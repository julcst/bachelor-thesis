@online{abrash1996,
  title = {Quake's {{Lighting Model}}:~ {{Surface Caching}}},
  author = {Abrash, Michael},
  date = {1996},
  url = {https://www.bluesnews.com/abrash/chap68.shtml},
  urldate = {2025-04-22},
  file = {/Users/julianstamm/Zotero/storage/UGNLCZC6/chap68.html}
}

@online{advancesrealtimerenderingcoms2021indexhtml,
  title = {Advances.Realtimerendering.Com/S2021/Index.Html},
  url = {https://advances.realtimerendering.com/s2021/index.html},
  urldate = {2025-08-23},
  file = {/Users/julianstamm/Zotero/storage/MJ9L6N4D/index.html}
}

@inproceedings{afra2019,
  title = {Open {{Image Denoise}}},
  author = {Áfra, Attila T.},
  date = {2019},
  publisher = {Intel Corporation},
  url = {https://www.highperformancegraphics.org/wp-content/uploads/2019/hot3d/open_image_denoise.pdf},
  urldate = {2025-08-25},
  file = {/Users/julianstamm/Zotero/storage/A8CELY8C/Áfra - 2019 - Open Image Denoise.pdf}
}

@unpublished{ahmed2019,
  title = {My {{Favorite Samples}}},
  author = {Ahmed, Abdalla and Keller, Alexander and Georgiev, Iliyan and Pharr, Matt and Christensen, Per},
  date = {2019},
  url = {https://www.youtube.com/},
  urldate = {2024-09-24},
  eventtitle = {{{SIGGRAPH Courses}}},
  langid = {english},
  venue = {Los Angeles, California},
  file = {/Users/julianstamm/Zotero/storage/KQ7ASC6B/watch.html}
}

@book{akenine-moller2019,
  title = {Real-Time Rendering},
  author = {Akenine-Moller, Tomas and Haines, Eric and Hoffman, Naty},
  date = {2019},
  publisher = {AK Peters/crc Press},
  url = {https://www.taylorfrancis.com/books/mono/10.1201/9781315365459/real-time-rendering-tomas-akenine-mo%CC%88ller-eric-haines-naty-hoffman},
  urldate = {2025-08-23},
  file = {/Users/julianstamm/Zotero/storage/LBA8HL25/Akenine-Moller et al. - 2019 - Real-time rendering.epub}
}

@article{andersson2020,
  title = {{{FLIP}}: {{A Difference Evaluator}} for {{Alternating Images}}.},
  shorttitle = {{{FLIP}}},
  author = {Andersson, Pontus and Nilsson, Jim and Akenine-Möller, Tomas and Oskarsson, Magnus and Åström, Kalle and Fairchild, Mark D.},
  date = {2020},
  journaltitle = {Proc. ACM Comput. Graph. Interact. Tech.},
  volume = {3},
  number = {2},
  pages = {15--1},
  url = {https://developer-blogs.nvidia.com/wp-content/uploads/2020/07/flip-author-version-reduced-file-size.pdf},
  urldate = {2025-08-25},
  file = {/Users/julianstamm/Zotero/storage/M28R76SE/Andersson et al. - 2020 - FLIP A Difference Evaluator for Alternating Images..pdf}
}

@inproceedings{andersson2021,
  title = {Visualizing {{Errors}} in {{Rendered High Dynamic Range Images}}},
  booktitle = {Eurographics Short Papers},
  author = {Andersson, Pontus and Nilsson, Jim and Shirley, Peter and Akenine-Möller, Tomas},
  date = {2021-05},
  doi = {10.2312/egs.20211015},
  url = {https://research.nvidia.com/publication/2021-05_visualizing-errors-rendered-high-dynamic-range-images},
  langid = {english},
  file = {/Users/julianstamm/Zotero/storage/IZPZ44RL/Andersson et al. - 2021 - Visualizing Errors in Rendered High Dynamic Range Images.pdf}
}

@report{andersson2024,
  title = {{{OpenPBR Surface Specification}}},
  author = {Andersson, Zap and Edmondson, Paul and Guertault, Julien and Herubel, Adrien and King, Alan and Kutz, Peter and Machizaud, Andréa and Portsmouth, Jamie and Servant, Frédéric and Stone, Jonathan},
  date = {2024},
  institution = {Academy Software Foundation (ASWF)},
  url = {https://academysoftwarefoundation.github.io/OpenPBR/}
}

@inproceedings{apers2024,
  title = {Shipping {{Dynamic Global Illumination}} in {{Frostbite}}},
  booktitle = {{{ACM SIGGRAPH}} 2024 {{Courses}}: {{Advances}} in {{Real-Time Rendering}} in {{Games}}},
  author = {Apers, Diede},
  date = {2024},
  url = {https://advances.realtimerendering.com/s2024/content/EA-GIBS2/Apers_Advances-s2024_Shipping-Dynamic-GI.pdf},
  urldate = {2025-05-04},
  eventtitle = {{{SIGGRAPH}} 2024},
  file = {/Users/julianstamm/Zotero/storage/IACBIS7H/Apers_Advances-s2024_Shipping-Dynamic-GI.pdf}
}

@inproceedings{appel1968,
  title = {Some Techniques for Shading Machine Renderings of Solids},
  booktitle = {Proceedings of the {{April}} 30--{{May}} 2, 1968, Spring Joint Computer Conference on - {{AFIPS}} '68 ({{Spring}})},
  author = {Appel, Arthur},
  date = {1968},
  pages = {37},
  publisher = {ACM Press},
  location = {Atlantic City, New Jersey},
  doi = {10.1145/1468075.1468082},
  url = {http://portal.acm.org/citation.cfm?doid=1468075.1468082},
  urldate = {2025-08-11},
  eventtitle = {The {{April}} 30--{{May}} 2, 1968, Spring Joint Computer Conference},
  langid = {english},
  file = {/Users/julianstamm/Zotero/storage/M9JWNYSQ/Appel - 1968 - Some techniques for shading machine renderings of solids.pdf}
}

@inproceedings{arikan2005,
  title = {Fast and Detailed Approximate Global Illumination by Irradiance Decomposition},
  booktitle = {{{ACM SIGGRAPH}} 2005 {{Papers}}},
  author = {Arikan, Okan and Forsyth, David A. and O'Brien, James F.},
  date = {2005-07},
  pages = {1108--1114},
  publisher = {ACM},
  location = {Los Angeles California},
  doi = {10.1145/1186822.1073319},
  url = {https://dl.acm.org/doi/10.1145/1186822.1073319},
  urldate = {2025-08-22},
  eventtitle = {{{SIGGRAPH05}}: {{Special Interest Group}} on {{Computer Graphics}} and {{Interactive Techniques Conference}}},
  isbn = {978-1-4503-7825-3},
  langid = {english},
  file = {/Users/julianstamm/Zotero/storage/7CS6W76R/Arikan et al. - 2005 - Fast and detailed approximate global illumination by irradiance decomposition.pdf}
}

@inproceedings{arvo1986,
  title = {Backward Ray Tracing},
  booktitle = {Developments in Ray Tracing, Computer Graphics, Proc. of {{ACM SIGGRAPH}} 86 Course Notes},
  author = {Arvo, James and others},
  date = {1986},
  pages = {259--263},
  file = {/Users/julianstamm/Zotero/storage/BYYX7CC2/Backward.pdf}
}

@article{barre-brisebois2017,
  title = {A {{Certain Slant}} of {{Light}}: {{Past}}, {{Present}} and {{Future Challenges}} of {{Global Illumination}} in {{Games}}},
  shorttitle = {A {{Certain Slant}} of {{Light}}},
  author = {Barré-Brisebois, Colin},
  date = {2017},
  journaltitle = {Lecturer on SIGGRAPH}
}

@inproceedings{barron2021,
  title = {Mip-{{NeRF}}: {{A Multiscale Representation}} for {{Anti-Aliasing Neural Radiance Fields}}},
  shorttitle = {Mip-{{NeRF}}},
  booktitle = {2021 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Barron, Jonathan T. and Mildenhall, Ben and Tancik, Matthew and Hedman, Peter and Martin-Brualla, Ricardo and Srinivasan, Pratul P.},
  date = {2021-10},
  pages = {5835--5844},
  publisher = {IEEE},
  location = {Montreal, QC, Canada},
  doi = {10.1109/ICCV48922.2021.00580},
  url = {https://ieeexplore.ieee.org/document/9710056/},
  urldate = {2025-04-23},
  abstract = {The rendering procedure used by neural radiance fields (NeRF) samples a scene with a single ray per pixel and may therefore produce renderings that are excessively blurred or aliased when training or testing images observe scene content at different resolutions. The straightforward solution of supersampling by rendering with multiple rays per pixel is impractical for NeRF, because rendering each ray requires querying a multilayer perceptron hundreds of times. Our solution, which we call “mip-NeRF” (`a la “mipmap”), extends NeRF to represent the scene at a continuously-valued scale. By efficiently rendering anti-aliased conical frustums instead of rays, mip-NeRF reduces objectionable aliasing artifacts and significantly improves NeRF’s ability to represent fine details, while also being 7\% faster than NeRF and half the size. Compared to NeRF, mip-NeRF reduces average error rates by 17\% on the dataset presented with NeRF and by 60\% on a challenging multiscale variant of that dataset that we present. Mip-NeRF is also able to match the accuracy of a brute-force supersampled NeRF on our multiscale dataset while being 22× faster.},
  eventtitle = {2021 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  isbn = {978-1-6654-2812-5},
  langid = {english},
  file = {/Users/julianstamm/Zotero/storage/P5SZJNF8/Barron et al. - 2021 - Mip-NeRF A Multiscale Representation for Anti-Aliasing Neural Radiance Fields.pdf}
}

@article{bauer2024,
  title = {Photon {{Field Networks}} for {{Dynamic Real-Time Volumetric Global Illumination}}},
  author = {Bauer, David and Wu, Qi and Ma, Kwan-Liu},
  date = {2024-01},
  journaltitle = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {30},
  number = {1},
  pages = {975--985},
  issn = {1941-0506},
  doi = {10.1109/TVCG.2023.3327107},
  url = {https://ieeexplore.ieee.org/document/10297590},
  urldate = {2025-05-13},
  abstract = {Volume data is commonly found in many scientific disciplines, like medicine, physics, and biology. Experts rely on robust scientific visualization techniques to extract valuable insights from the data. Recent years have shown path tracing to be the preferred approach for volumetric rendering, given its high levels of realism. However, real-time volumetric path tracing often suffers from stochastic noise and long convergence times, limiting interactive exploration. In this paper, we present a novel method to enable real-time global illumination for volume data visualization. We develop Photon Field Networks—a phase-function-aware, multi-light neural representation of indirect volumetric global illumination. The fields are trained on multi-phase photon caches that we compute a priori. Training can be done within seconds, after which the fields can be used in various rendering tasks. To showcase their potential, we develop a custom neural path tracer, with which our photon fields achieve interactive framerates even on large datasets. We conduct in-depth evaluations of the method's performance, including visual quality, stochastic noise, inference and rendering speeds, and accuracy regarding illumination and phase function awareness. Results are compared to ray marching, path tracing and photon mapping. Our findings show that Photon Field Networks can faithfully represent indirect global illumination within the boundaries of the trained phase spectrum while exhibiting less stochastic noise and rendering at a significantly faster rate than traditional methods.},
  keywords = {Data visualization,deep learning,global illumination,Light sources,Lighting,neural rendering,path tracing,Photonics,Real-time systems,Rendering (computer graphics),Visualization,Volume data,volume rendering,volume visualization},
  file = {/Users/julianstamm/Zotero/storage/WCS3UR6R/Bauer et al. - 2024 - Photon Field Networks for Dynamic Real-Time Volumetric Global Illumination.pdf;/Users/julianstamm/Zotero/storage/RMJ3ZGPB/10297590.html}
}

@article{bekaert2003,
  title = {A Custom Designed Density Estimation Method for Light Transport},
  author = {Bekaert, Philippe and Slusallek, Philipp and Cools, Ronald and Havran, Vlastimil and Seidel, Hans-Peter},
  date = {2003},
  publisher = {Max-Planck-Institut für Informatik},
  url = {https://pure.mpg.de/pubman/faces/ViewItemOverviewPage.jsp?itemId=item_1819214},
  urldate = {2025-08-03},
  file = {/Users/julianstamm/Zotero/storage/4IQ6C3AU/Bekaert et al. - 2003 - A custom designed density estimation method for light transport.pdf}
}

@article{belcour2013,
  title = {{{5D Covariance}} Tracing for Efficient Defocus and Motion Blur},
  author = {Belcour, Laurent and Soler, Cyril and Subr, Kartic and Holzschuch, Nicolas and Durand, Fredo},
  date = {2013-07-04},
  journaltitle = {ACM Trans. Graph.},
  volume = {32},
  number = {3},
  pages = {31:1--31:18},
  issn = {0730-0301},
  doi = {10.1145/2487228.2487239},
  url = {https://dl.acm.org/doi/10.1145/2487228.2487239},
  urldate = {2025-08-03},
  abstract = {The rendering of effects such as motion blur and depth-of-field requires costly 5D integrals. We accelerate their computation through adaptive sampling and reconstruction based on the prediction of the anisotropy and bandwidth of the integrand. For this, we develop a new frequency analysis of the 5D temporal light-field, and show that first-order motion can be handled through simple changes of coordinates in 5D. We further introduce a compact representation of the spectrum using the covariance matrix and Gaussian approximations. We derive update equations for the 5 × 5 covariance matrices for each atomic light transport event, such as transport, occlusion, BRDF, texture, lens, and motion. The focus on atomic operations makes our work general, and removes the need for special-case formulas. We present a new rendering algorithm that computes 5D covariance matrices on the image plane by tracing paths through the scene, focusing on the single-bounce case. This allows us to reduce sampling rates when appropriate and perform reconstruction of images with complex depth-of-field and motion blur effects.},
  file = {/Users/julianstamm/Zotero/storage/JND5UK7F/Belcour et al. - 2013 - 5D Covariance tracing for efficient defocus and motion blur.pdf}
}

@online{benyoub2024,
  title = {{{VNDF}} Importance Sampling for an Isotropic {{Smith-GGX}} Distribution},
  author = {Benyoub, Anis},
  date = {2024-04-15T13:59:17+00:00},
  url = {https://auzaiffe.wordpress.com/2024/04/15/vndf-importance-sampling-an-isotropic-distribution/},
  urldate = {2024-08-28},
  abstract = {In this blog post, you will find an implementation for importance sampling a VNDF (GGX-Smith) isotropic distribution that is 15\% faster than the current state of the art and doesn’t require b…},
  langid = {english},
  organization = {A journey into rendering},
  keywords = {Important},
  file = {/Users/julianstamm/Zotero/storage/GL7VG6SL/vndf-importance-sampling-an-isotropic-distribution.html}
}

@inproceedings{binder2018,
  title = {Fast Path Space Filtering by Jittered Spatial Hashing},
  booktitle = {{{ACM SIGGRAPH}} 2018 {{Talks}}},
  author = {Binder, Nikolaus and Fricke, Sascha and Keller, Alexander},
  date = {2018-08-12},
  series = {{{SIGGRAPH}} '18},
  pages = {1--2},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3214745.3214806},
  url = {https://dl.acm.org/doi/10.1145/3214745.3214806},
  urldate = {2025-06-27},
  abstract = {Restricting path tracing to a small number of paths per pixel for performance reasons rarely achieves a satisfactory image quality for scenes of interest. However, path space filtering may dramatically improve the visual quality by sharing information across vertices of paths classified as "nearby". While thus contributions can be filtered in path space and beyond the first intersection, searching "nearby" paths is more expensive than filtering in screen space. We greatly improve over this performance penalty by storing and looking up the required information in a hash map using hash keys constructed from jittered and quantized information, such that only a single query may replace costly neighborhood searches.},
  isbn = {978-1-4503-5820-0},
  file = {/Users/julianstamm/Zotero/storage/B4U7KQ2G/Binder et al. - 2018 - Fast path space filtering by jittered spatial hashing.pdf}
}

@online{binder2021,
  title = {Massively {{Parallel Path Space Filtering}}},
  author = {Binder, Nikolaus and Fricke, Sascha and Keller, Alexander},
  date = {2021-02-03},
  eprint = {1902.05942},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1902.05942},
  url = {http://arxiv.org/abs/1902.05942},
  urldate = {2025-08-25},
  abstract = {Restricting path tracing to a small number of paths per pixel for performance reasons rarely achieves a satisfactory image quality for scenes of interest. However, path space filtering may dramatically improve the visual quality by sharing information across vertices of paths classified as proximate. Unlike screen space-based approaches, these paths neither need to be present on the screen, nor is filtering restricted to the first intersection with the scene. While searching proximate vertices had been more expensive than filtering in screen space, we greatly improve over this performance penalty by storing, updating, and looking up the required information in a hash table. The keys are constructed from jittered and quantized information, such that only a single query very likely replaces costly neighborhood searches. A massively parallel implementation of the algorithm is demonstrated on a graphics processing unit (GPU).},
  pubstate = {prepublished},
  keywords = {Computer Science - Graphics},
  file = {/Users/julianstamm/Zotero/storage/PUBRTVM8/Binder et al. - 2021 - Massively Parallel Path Space Filtering.pdf;/Users/julianstamm/Zotero/storage/NDQM4W5L/1902.html}
}

@thesis{bitterli2013,
  title = {{{BSSRDF Explorer}}: {{A Rendering Framework}} for the {{BSSRDF}}},
  author = {Bitterli, Benedikt},
  date = {2013-04},
  langid = {english},
  file = {/Users/julianstamm/Zotero/storage/MXJ8XKGQ/Bitterli - BSSRDF Explorer A Rendering Framework for the BSSRDF.pdf}
}

@article{bitterli2020,
  title = {Spatiotemporal Reservoir Resampling for Real-Time Ray Tracing with Dynamic Direct Lighting},
  author = {Bitterli, Benedikt and Wyman, Chris and Pharr, Matt and Shirley, Peter and Lefohn, Aaron and Jarosz, Wojciech},
  date = {2020-08-31},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {39},
  number = {4},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/3386569.3392481},
  url = {https://dl.acm.org/doi/10.1145/3386569.3392481},
  urldate = {2025-06-25},
  abstract = {Efficiently rendering direct lighting from millions of dynamic light sources using Monte Carlo integration remains a challenging problem, even for off-line rendering systems. We introduce a new algorithm---ReSTIR---that renders such lighting interactively, at high quality, and without needing to maintain complex data structures. We repeatedly resample a set of candidate light samples and apply further spatial and temporal resampling to leverage information from relevant nearby samples. We derive an unbiased Monte Carlo estimator for this approach, and show that it achieves equal-error 6×-60× faster than state-of-the-art methods. A biased estimator reduces noise further and is 35×-65× faster, at the cost of some energy loss. We implemented our approach on the GPU, rendering complex scenes containing up to 3.4 million dynamic, emissive triangles in under 50 ms per frame while tracing at most 8 rays per pixel.},
  langid = {english},
  file = {/Users/julianstamm/Zotero/storage/MF9HIG9S/Bitterli et al. - 2020 - Spatiotemporal reservoir resampling for real-time ray tracing with dynamic direct lighting.pdf;/Users/julianstamm/Zotero/storage/M8NBIH3M/restir.html}
}

@online{blenderfoundation,
  title = {Sampling {{Patterns}} - {{Blender Developer Documentation}}},
  author = {{Blender Foundation}},
  url = {https://developer.blender.org/docs/features/cycles/sampling_patterns/},
  urldate = {2025-06-15},
  file = {/Users/julianstamm/Zotero/storage/ZJ4PLDZQ/sampling_patterns.html}
}

@online{boisse2023,
  title = {{{GI-1}}.0: {{A Fast}} and {{Scalable Two-level Radiance Caching Scheme}} for {{Real-time Global Illumination}}},
  shorttitle = {{{GI-1}}.0},
  author = {Boissé, Guillaume and Meunier, Sylvain and family=Dinechin, given=Heloise, prefix=de, useprefix=false and Bartels, Pieterjan and Veselov, Alexander and Eto, Kenta and Harada, Takahiro},
  date = {2023-10-30},
  eprint = {2310.19855},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2310.19855},
  url = {http://arxiv.org/abs/2310.19855},
  urldate = {2025-08-02},
  abstract = {Real-time global illumination is key to enabling more dynamic and physically realistic worlds in performance-critical applications such as games or any other applications with real-time constraints.Hardware-accelerated ray tracing in modern GPUs allows arbitrary intersection queries against the geometry, making it possible to evaluate indirect lighting entirely at runtime. However, only a small number of rays can be traced at each pixel to maintain high framerates at ever-increasing image resolutions. Existing solutions, such as probe-based techniques, approximate the irradiance signal at the cost of a few rays per frame but suffer from a lack of details and slow response times to changes in lighting. On the other hand, reservoir-based resampling techniques capture much more details but typically suffer from poorer performance and increased amounts of noise, making them impractical for the current generation of hardware and gaming consoles. To find a balance that achieves high lighting fidelity while maintaining a low runtime cost, we propose a solution that dynamically estimates global illumination without needing any content preprocessing, thus enabling easy integration into existing real-time rendering pipelines.},
  pubstate = {prepublished},
  keywords = {Computer Science - Graphics},
  file = {/Users/julianstamm/Zotero/storage/IR4GF924/Boissé et al. - 2023 - GI-1.0 A Fast and Scalable Two-level Radiance Caching Scheme for Real-time Global Illumination.pdf;/Users/julianstamm/Zotero/storage/BH5FLED4/2310.html}
}

@unpublished{boksanksky2023,
  title = {Advancing {{Real-Time Path Tracing}} with {{Neural Radiance Cache}} | {{GTC Digital Spring}} 2023 | {{NVIDIA On-Demand}}},
  author = {Boksanksky, Jakub and Mihuț, Ana},
  date = {2023},
  url = {https://www.nvidia.com/en-us/on-demand/session/gtcspring23-s51967/},
  urldate = {2025-03-22},
  abstract = {We'll discuss the new Neural Radiance Cache (NRC) technology, AI-based radiance caching to rendering applications, based on research carried out at NVIDIA},
  langid = {american},
  file = {/Users/julianstamm/Zotero/storage/NYYXXH4B/Advancing Real-Time Path Tracing with Neural Radiance Cache  GTC Digital Spring 2023  NVIDIA On-De.pdf;/Users/julianstamm/Zotero/storage/XYNTXBG9/gtcspring23-s51967.html}
}

@thesis{bruin2025,
  type = {phdthesis},
  title = {Global {{Illumination}} Using {{ReSTIR DI}} and {{Photon-Mapped Virtual Point Lights}}},
  author = {Bruin, Samuel},
  date = {2025},
  institution = {Delft University of Technology},
  url = {https://repository.tudelft.nl/record/uuid:ddb7ccdd-289b-48a4-bffa-541840f9cfbf},
  urldate = {2025-08-21},
  file = {/Users/julianstamm/Zotero/storage/3M83JNWN/Bruin - 2025 - Global Illumination using ReSTIR DI and Photon-Mapped Virtual Point Lights.pdf}
}

@inproceedings{burley2012,
  title = {Physically-{{Based Shading}} at {{Disney}}},
  booktitle = {Acm {{Siggraph}}},
  author = {Burley, Brent and {Walt Disney Animation Studios}},
  date = {2012},
  volume = {2012},
  number = {2012},
  pages = {1--7},
  publisher = {vol. 2012},
  url = {https://cw.fel.cvut.cz/b241/_media/courses/b4m39rso/lectures/s2012_pbs_disney_brdf_notes_v3.pdf},
  urldate = {2025-06-21},
  eventtitle = {{{SIGGRAPH}}},
  langid = {english},
  file = {/Users/julianstamm/Zotero/storage/9NQM5I7N/Burley and Studios - 2012 - Physically-based shading at disney.pdf}
}

@article{burley2020,
  title = {Practical {{Hash-based Owen Scrambling}}},
  author = {Burley, Brent},
  date = {2020},
  journaltitle = {Journal of Computer Graphics Techniques},
  shortjournal = {JCGT},
  volume = {10},
  number = {4},
  pages = {29},
  url = {https://jcgt.org/published/0009/04/01/paper.pdf},
  abstract = {Owen’s nested uniform scrambling maximally randomizes low-discrepancy sequences while preserving multidimensional stratification. This enables advantageous convergence for favorable integrands and bounded error for unfavorable ones, and makes it less prone to structured artifacts than other scrambling methods. The Owen-scrambled Sobol sequence in particular has been gaining popularity recently in computer graphics. However, implementations typically use a precomputed table of samples which imposes limits on sequence length and dimension.},
  langid = {english},
  file = {/Users/julianstamm/Zotero/storage/KLRTTVCW/Burley - 2020 - Practical Hash-based Owen Scrambling.pdf}
}

@online{cao2016,
  title = {Image {{Based Lighting}} in {{Offline}} and {{Real-time Rendering}}},
  author = {Cao, Jiayin},
  date = {2016-09-07T00:00:00+00:00},
  url = {https://agraphicsguynotes.com/posts/image_based_lighting_in_offline_and_realtime_rendering/},
  urldate = {2024-08-28},
  abstract = {Image-based lighting is a practical way to enhance the~visual quality of computer graphics. I used to be confused by it until I read the book “High Dynamic Range Imaging”, which provides a very clear explanation about IBL. And I actually have implemented the algorithm in my offline renderer before, it was just that I didn’t know it is IBL. The book PBRT has some materials talking about it without explicitly mentioning the term.},
  langid = {english},
  organization = {A GRAPHICS GUY'S NOTE},
  file = {/Users/julianstamm/Zotero/storage/XXEZUICZ/image_based_lighting_in_offline_and_realtime_rendering.html}
}

@article{chaitanya2017,
  title = {Interactive Reconstruction of {{Monte Carlo}} Image Sequences Using a Recurrent Denoising Autoencoder},
  author = {Chaitanya, Chakravarty R. Alla and Kaplanyan, Anton S. and Schied, Christoph and Salvi, Marco and Lefohn, Aaron and Nowrouzezahrai, Derek and Aila, Timo},
  date = {2017-08-31},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {36},
  number = {4},
  pages = {1--12},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/3072959.3073601},
  url = {https://dl.acm.org/doi/10.1145/3072959.3073601},
  urldate = {2025-08-25},
  abstract = {We describe a machine learning technique for reconstructing image sequences rendered using Monte Carlo methods. Our primary focus is on reconstruction of global illumination with extremely low sampling budgets at interactive rates. Motivated by recent advances in image restoration with deep convolutional networks, we propose a variant of these networks better suited to the class of noise present in Monte Carlo rendering. We allow for much larger pixel neighborhoods to be taken into account, while also improving execution speed by an order of magnitude. Our primary contribution is the addition of recurrent connections to the network in order to drastically improve temporal stability for sequences of sparsely sampled input images. Our method also has the desirable property of automatically modeling relationships based on auxiliary per-pixel input channels, such as depth and normals. We show significantly higher quality results compared to existing methods that run at comparable speeds, and furthermore argue a clear path for making our method run at realtime rates in the near future.},
  langid = {english},
  file = {/Users/julianstamm/Zotero/storage/UWFQTT4R/Chaitanya et al. - 2017 - Interactive reconstruction of Monte Carlo image sequences using a recurrent denoising autoencoder.pdf}
}

@article{chen2011,
  title = {Improved {{Stochastic Progressive Photon Mapping}} with {{Metropolis Sampling}}},
  author = {Chen, Jiating and Wang, Bin and Yong, Jun-Hai},
  date = {2011},
  journaltitle = {Computer Graphics Forum},
  volume = {30},
  number = {4},
  pages = {1205--1213},
  issn = {1467-8659},
  doi = {10.1111/j.1467-8659.2011.01979.x},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-8659.2011.01979.x},
  urldate = {2025-05-18},
  abstract = {This paper presents an improvement to the stochastic progressive photon mapping (SPPM), a method for robustly simulating complex global illumination with distributed ray tracing effects. Normally, similar to photon mapping and other particle tracing algorithms, SPPM would become inefficient when the photons are poorly distributed. An inordinate amount of photons are required to reduce the error caused by noise and bias to acceptable levels. In order to optimize the distribution of photons, we propose an extension of SPPM with a Metropolis-Hastings algorithm, effectively exploiting local coherence among the light paths that contribute to the rendered image. A well-designed scalar contribution function is introduced as our Metropolis sampling strategy, targeting at specific parts of image areas with large error to improve the efficiency of the radiance estimator. Experimental results demonstrate that the new Metropolis sampling based approach maintains the robustness of the standard SPPM method, while significantly improving the rendering efficiency for a wide range of scenes with complex lighting.},
  langid = {english},
  keywords = {I.3.3 Computer Graphics: Picture/Image Generation,I.3.7 Computer Graphics: Three-Dimensional Graphics and Realism},
  file = {/Users/julianstamm/Zotero/storage/PM2HAMZI/j.1467-8659.2011.01979.html}
}

@online{cheng2021,
  title = {Go {{Small}} and {{Similar}}: {{A Simple Output Decay Brings Better Performance}}},
  shorttitle = {Go {{Small}} and {{Similar}}},
  author = {Cheng, Xuan and Xie, Tianshu and Wang, Xiaomin and Deng, Jiali and Liu, Minghui and Liu, Ming},
  date = {2021-06-12},
  eprint = {2106.06726},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2106.06726},
  url = {http://arxiv.org/abs/2106.06726},
  urldate = {2025-05-04},
  abstract = {Regularization and data augmentation methods have been widely used and become increasingly indispensable in deep learning training. Researchers who devote themselves to this have considered various possibilities. But so far, there has been little discussion about regularizing outputs of the model. This paper begins with empirical observations that better performances are significantly associated with output distributions, that have smaller average values and variances. By audaciously assuming there is causality involved, we propose a novel regularization term, called Output Decay, that enforces the model to assign smaller and similar output values on each class. Though being counter-intuitive, such a small modification result in a remarkable improvement on performance. Extensive experiments demonstrate the wide applicability, versatility, and compatibility of Output Decay.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/julianstamm/Zotero/storage/APRRMMY7/Cheng et al. - 2021 - Go Small and Similar A Simple Output Decay Brings Better Performance.pdf;/Users/julianstamm/Zotero/storage/8PJW58AD/2106.html}
}

@inproceedings{christensen2004,
  title = {An {{Irradiance Atlas}} for {{Global Illumination}} in {{Complex Production Scenes}}.},
  booktitle = {Rendering {{Techniques}}},
  author = {Christensen, Per H. and Batali, Dana},
  date = {2004},
  pages = {133--142},
  file = {/Users/julianstamm/Zotero/storage/MUTEWTFD/Christensen and Batali - 2004 - An Irradiance Atlas for Global Illumination in Complex Production Scenes..pdf}
}

@article{christensen2016,
  title = {The {{Path}} to {{Path-Traced Movies}}},
  author = {Christensen, Per H. and Jarosz, Wojciech},
  date = {2016},
  journaltitle = {Foundations and Trends® in Computer Graphics and Vision},
  volume = {10},
  number = {2},
  pages = {103--175},
  publisher = {Now Publishers, Inc.},
  url = {https://graphics.pixar.com/library/PathTracedMovies/paper.pdf},
  urldate = {2025-08-11},
  file = {/Users/julianstamm/Zotero/storage/454MW4IH/paper.pdf}
}

@article{cigolle2014,
  title = {A {{Survey}} of {{Efﬁcient Representations}} for {{Independent Unit Vectors}}},
  author = {Cigolle, Zina H and Donow, Sam and Evangelakos, Daniel and Mara, Michael and McGuire, Morgan and Meyer, Quirin},
  date = {2014},
  journaltitle = {Journal of Computer Graphics Techniques},
  shortjournal = {Journal of Computer Graphics Techniques},
  volume = {3},
  number = {2},
  issn = {2331-7418},
  url = {https://jcgt.org/published/0003/02/01/paper-lowres.pdf},
  abstract = {The bandwidth cost and memory footprint of vector buffers are limiting factors for GPU rendering in many applications. This article surveys time-and space-efficient representations for the important case of non-register, in-core, statistically independent unit vectors, with emphasis on GPU encoding and decoding. These representations are appropriate for unit vectors in a geometry buffer or attribute stream—where no correlation between adjacent vectors is easily available—or for those in a normal map where quality higher than that of DXN is required. We do not address out-of-core and register storage vectors because they favor minimum-space and maximum-speed alternatives, respectively.},
  file = {/Users/julianstamm/Zotero/storage/INEEC6AN/Cigolle et al. - 2014 - A Survey of Efﬁcient Representations for Independent Unit Vectors.pdf;/Users/julianstamm/Zotero/storage/KZV4UPG7/Cigolle et al. - 2014 - A Survey of Efﬁcient Representations for Independent Unit Vectors.pdf}
}

@inproceedings{cohen1988,
  title = {A Progressive Refinement Approach to Fast Radiosity Image Generation},
  booktitle = {Proceedings of the 15th Annual Conference on {{Computer}} Graphics and Interactive Techniques},
  author = {Cohen, Michael F. and Chen, Shenchang Eric and Wallace, John R. and Greenberg, Donald P.},
  date = {1988-06},
  pages = {75--84},
  publisher = {ACM},
  doi = {10.1145/54852.378487},
  url = {https://dl.acm.org/doi/10.1145/54852.378487},
  urldate = {2025-08-22},
  eventtitle = {{{SIGGRAPH88}}: 15th {{Annual}} Conference on {{Computer Graphics}}},
  isbn = {978-0-89791-275-4},
  langid = {english},
  file = {/Users/julianstamm/Zotero/storage/D4ECHCPZ/Cohen et al. - 1988 - A progressive refinement approach to fast radiosity image generation.pdf}
}

@online{colbert,
  title = {Chapter 20. {{GPU-Based Importance Sampling}}},
  author = {Colbert, Mark and Křivánek, Jaroslav},
  url = {https://developer.nvidia.com/gpugems/gpugems3/part-iii-rendering/chapter-20-gpu-based-importance-sampling},
  urldate = {2024-08-28},
  langid = {american},
  organization = {NVIDIA Developer},
  file = {/Users/julianstamm/Zotero/storage/25X6D5L2/chapter-20-gpu-based-importance-sampling.html}
}

@article{cook1982,
  title = {A {{Reflectance Model}} for {{Computer Graphics}}},
  author = {Cook, R. L. and Torrance, K. E.},
  date = {1982-01},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {1},
  number = {1},
  pages = {7--24},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/357290.357293},
  url = {https://dl.acm.org/doi/10.1145/357290.357293},
  urldate = {2024-02-23},
  langid = {english},
  file = {/Users/julianstamm/Zotero/storage/KCIYIXWX/Cook and Torrance - 1982 - A Reflectance Model for Computer Graphics.pdf}
}

@article{cranley1976,
  title = {Randomization of {{Number Theoretic Methods}} for {{Multiple Integration}}},
  author = {Cranley, R. and Patterson, T. N. L.},
  date = {1976},
  journaltitle = {SIAM Journal on Numerical Analysis},
  volume = {13},
  number = {6},
  eprint = {2156452},
  eprinttype = {jstor},
  pages = {904--914},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {0036-1429},
  url = {https://www.jstor.org/stable/2156452},
  urldate = {2025-06-15},
  abstract = {A procedure is discussed for randomization of the number theoretic methods of the Korobov type producing stochastic families of multi-dimensional integration rules. These randomized rules have the advantage that confidence intervals can be given for the magnitude of error. The practical implementation is considered.}
}

@article{crassin2011,
  title = {Interactive {{Indirect Illumination Using Voxel Cone Tracing}}},
  author = {Crassin, Cyril and Neyret, Fabrice and Sainz, Miguel and Green, Simon and Eisemann, Elmar},
  date = {2011-09},
  journaltitle = {Computer Graphics Forum},
  shortjournal = {Computer Graphics Forum},
  volume = {30},
  number = {7},
  pages = {1921--1930},
  issn = {0167-7055, 1467-8659},
  doi = {10.1111/j.1467-8659.2011.02063.x},
  url = {https://onlinelibrary.wiley.com/doi/10.1111/j.1467-8659.2011.02063.x},
  urldate = {2025-08-25},
  abstract = {Abstract             Indirect illumination is an important element for realistic image synthesis, but its computation is expensive and highly dependent on the complexity of the scene and of the BRDF of the involved surfaces. While off‐line computation and pre‐baking can be acceptable for some cases, many applications (games, simulators, etc.) require real‐time or interactive approaches to evaluate indirect illumination. We present a novel algorithm to compute indirect lighting in real‐time that avoids costly precomputation steps and is not restricted to low‐frequency illumination. It is based on a hierarchical voxel octree representation generated and updated on the fly from a regular scene mesh coupled with an approximate voxel cone tracing that allows for a fast estimation of the visibility and incoming energy. Our approach can manage two light bounces for both Lambertian and glossy materials at interactive framerates (25–70FPS). It exhibits an almost scene‐independent performance and can handle complex scenes with dynamic content thanks to an interactive octree‐voxelization scheme. In addition, we demonstrate that our voxel cone tracing can be used to efficiently estimate Ambient Occlusion.},
  langid = {english},
  file = {/Users/julianstamm/Zotero/storage/B7TJSRGC/Crassin et al. - Interactive Indirect Illumination Using Voxel Cone Tracing.pdf;/Users/julianstamm/Zotero/storage/GEUM9JW7/Crassin et al. - 2011 - Interactive Indirect Illumination Using Voxel Cone Tracing.pdf}
}

@inproceedings{dahm2017,
  title = {Learning Light Transport the Reinforced Way},
  booktitle = {{{ACM SIGGRAPH}} 2017 {{Talks}}},
  author = {Dahm, Ken and Keller, Alexander},
  date = {2017-07-30},
  pages = {1--2},
  publisher = {ACM},
  location = {Los Angeles California},
  doi = {10.1145/3084363.3085032},
  url = {https://dl.acm.org/doi/10.1145/3084363.3085032},
  urldate = {2025-08-22},
  eventtitle = {{{SIGGRAPH}} '17: {{Special Interest Group}} on {{Computer Graphics}} and {{Interactive Techniques Conference}}},
  isbn = {978-1-4503-5008-2},
  langid = {english},
  file = {/Users/julianstamm/Zotero/storage/KIUV5LKP/Dahm and Keller - 2017 - Learning light transport the reinforced way.pdf;/Users/julianstamm/Zotero/storage/TTW32DLN/Dahm and Keller - 2017 - Learning light transport the reinforced way.pdf}
}

@online{dereviannykh2024,
  title = {Neural {{Two-Level Monte Carlo Real-Time Rendering}}},
  author = {Dereviannykh, Mikhail and Klepikov, Dmitrii and Hanika, Johannes and Dachsbacher, Carsten},
  date = {2024-12-05},
  eprint = {2412.04634},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2412.04634},
  url = {http://arxiv.org/abs/2412.04634},
  urldate = {2025-05-10},
  abstract = {We introduce an efficient Two-Level Monte Carlo (subset of Multi-Level Monte Carlo, MLMC) estimator for real-time rendering of scenes with global illumination. Using MLMC we split the shading integral into two parts: the radiance cache integral and the residual error integral that compensates for the bias of the first one. For the first part, we developed the Neural Incident Radiance Cache (NIRC) leveraging the power of fully-fused tiny neural networks as a building block, which is trained on the fly. The cache is designed to provide a fast and reasonable approximation of the incident radiance: an evaluation takes 2-25x less compute time than a path tracing sample. This enables us to estimate the radiance cache integral with a high number of samples and by this achieve faster convergence. For the residual error integral, we compute the difference between the NIRC predictions and the unbiased path tracing simulation. Our method makes no assumptions about the geometry, materials, or lighting of a scene and has only few intuitive hyper-parameters. We provide a comprehensive comparative analysis in different experimental scenarios. Since the algorithm is trained in an on-line fashion, it demonstrates significant noise level reduction even for dynamic scenes and can easily be combined with other importance sampling schemes and noise reduction techniques.},
  pubstate = {prepublished},
  version = {1},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Graphics},
  file = {/Users/julianstamm/Zotero/storage/V8LRZ6RQ/Dereviannykh et al. - 2024 - Neural Two-Level Monte Carlo Real-Time Rendering.pdf;/Users/julianstamm/Zotero/storage/TYAISTK5/2412.html}
}

@article{ding2023,
  title = {Approximate Global Illumination Using Photon Mapping: {{A}} Review},
  shorttitle = {Approximate Global Illumination Using Photon Mapping},
  author = {Ding, Haochen},
  date = {2023-12-31},
  journaltitle = {Highlights in Science, Engineering and Technology},
  shortjournal = {Highlights in Science, Engineering and Technology},
  volume = {76},
  pages = {799--813},
  doi = {10.54097/cja69975},
  abstract = {This paper introduces the two-step process of the standard photon mapping algorithm and a variety of other global biased illumination algorithms using photon mapping that have been proposed in recent years. Since there are many global biased illumination algorithms based on photon mapping, this review is a combination of these algorithms for the convenience of the reader, and the two-step process of standard PM will also be presented in this review to accommodate readers who are not familiar with the field. We will first introduce the specific implementation process and principles based on the two steps of PM, photon tracking and rendering, and then introduce a variety of polarized illumination techniques using photon maps, compare these methods and discuss the advantages, disadvantages, and applicability of each method in detail, and finally give a specific implementation of each method to give readers a clearer understanding of these methods. The final implementation of each method is shown to give the reader a clearer understanding of these methods.},
  file = {/Users/julianstamm/Zotero/storage/Y2FJ5JVL/Ding - 2023 - Approximate global illumination using photon mapping A review.pdf}
}

@inproceedings{dong2023,
  title = {Neural {{Parametric Mixtures}} for {{Path Guiding}}},
  booktitle = {Special {{Interest Group}} on {{Computer Graphics}} and {{Interactive Techniques Conference Conference Proceedings}}},
  author = {Dong, Honghao and Wang, Guoping and Li, Sheng},
  date = {2023-07-23},
  eprint = {2504.04315},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {1--10},
  doi = {10.1145/3588432.3591533},
  url = {http://arxiv.org/abs/2504.04315},
  urldate = {2025-05-10},
  abstract = {Previous path guiding techniques typically rely on spatial subdivision structures to approximate directional target distributions, which may cause failure to capture spatio-directional correlations and introduce parallax issue. In this paper, we present Neural Parametric Mixtures (NPM), a neural formulation to encode target distributions for path guiding algorithms. We propose to use a continuous and compact neural implicit representation for encoding parametric models while decoding them via lightweight neural networks. We then derive a gradient-based optimization strategy to directly train the parameters of NPM with noisy Monte Carlo radiance estimates. Our approach efficiently models the target distribution (incident radiance or the product integrand) for path guiding, and outperforms previous guiding methods by capturing the spatio-directional correlations more accurately. Moreover, our approach is more training efficient and is practical for parallelization on modern GPUs.},
  keywords = {Computer Science - Graphics},
  file = {/Users/julianstamm/Zotero/storage/5R9EESXN/Dong et al. - 2023 - Neural Parametric Mixtures for Path Guiding.pdf}
}

@software{dupuy,
  title = {Sampling {{Visible GGX Normals}} with {{Spherical Caps}} - {{Github Gist}}},
  author = {Dupuy, Jonathan},
  url = {https://gist.github.com/jdupuy/4c6e782b62c92b9cb3d13fbb0a5bd7a0},
  urldate = {2024-08-28},
  abstract = {Sampling Visible GGX Normals with Spherical Caps. GitHub Gist: instantly share code, notes, and snippets.},
  file = {/Users/julianstamm/Zotero/storage/N4SMYDIH/4c6e782b62c92b9cb3d13fbb0a5bd7a0.html}
}

@online{dupuy2023,
  title = {Sampling {{Visible GGX Normals}} with {{Spherical Caps}}},
  author = {Dupuy, Jonathan and Benyoub, Anis},
  date = {2023-06-12},
  eprint = {2306.05044},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2306.05044},
  urldate = {2024-08-28},
  abstract = {Importance sampling the distribution of visible GGX normals requires sampling those of a hemisphere. In this work, we introduce a novel method for sampling such visible normals. Our method builds upon the insight that a hemispherical mirror reflects parallel light rays uniformly within a solid angle shaped as a spherical cap. This spherical cap has the same apex as the hemispherical mirror, and its aperture given by the angle formed by the orientation of that apex and the direction of incident light rays. Based on this insight, we sample GGX visible normals as halfway vectors between a given incident direction and directions drawn from its associated spherical cap. Our resulting implementation is even simpler than that of Heitz and leads to up to systematic speed-ups in our benchmarks.},
  pubstate = {prepublished},
  keywords = {Computer Science - Graphics},
  file = {/Users/julianstamm/Zotero/storage/4RQM7LNR/Dupuy and Benyoub - 2023 - Sampling Visible GGX Normals with Spherical Caps.pdf;/Users/julianstamm/Zotero/storage/W77TVJH5/2306.html}
}

@online{dynamicdiffuseglobalillumination,
  title = {Dynamic {{Diffuse Global Illumination}}},
  url = {https://morgan3d.github.io/articles/2019-04-01-ddgi/},
  urldate = {2025-07-29},
  file = {/Users/julianstamm/Zotero/storage/JD7YBCX5/2019-04-01-ddgi.html}
}

@online{dynamicdiffuseglobalilluminationraytracedirradiancefieldsjcgt,
  title = {Dynamic {{Diffuse Global Illumination}} with {{Ray-Traced Irradiance Fields}} ({{JCGT}})},
  url = {https://jcgt.org/published/0008/02/01/},
  urldate = {2025-07-29},
  file = {/Users/julianstamm/Zotero/storage/LQVEMFD4/01.html}
}

@software{embarkstudioskajiya2025,
  title = {{{EmbarkStudios}}/Kajiya},
  date = {2025-08-19T13:32:05Z},
  origdate = {2021-03-26T03:18:43Z},
  url = {https://github.com/EmbarkStudios/kajiya},
  urldate = {2025-08-23},
  abstract = {💡 Experimental real-time global illumination renderer 🦀},
  organization = {Embark}
}

@inproceedings{engelhardt2008,
  title = {Octahedron {{Environment Maps}}.},
  booktitle = {{{VMV}}},
  author = {Engelhardt, Thomas and Dachsbacher, Carsten},
  date = {2008},
  pages = {383--388},
  publisher = {Citeseer},
  url = {https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=fcb9a6dbdf7b4c31f94e481cf101c83b73ea6410},
  urldate = {2024-12-21},
  file = {/Users/julianstamm/Zotero/storage/77HNC9NN/Engelhardt and Dachsbacher - 2008 - Octahedron Environment Maps..pdf}
}

@article{evangelou2021,
  title = {Fast {{Radius Search Exploiting Ray-Tracing Frameworks}}, {{JCGT}}},
  author = {Evangelou, Iordanis and Papaioannou, Georgios and Vardis, Konstantinos and Vasilakis, Andreas},
  date = {2021-02-05},
  volume = {10},
  pages = {2021},
  url = {https://jcgt.org/published/0010/01/02/},
  abstract = {Spatial queries to infer information from the neighborhood of a set of points are very frequently performed in rendering and geometry processing algorithms. Traditionally, these are accomplished using radius and k-nearest neighbors search operations, which utilize kd-trees and other specialized spatial data structures that fall short of delivering high performance. Recently, advances in ray tracing performance, with respect to both acceleration data structure construction and ray traversal times, have resulted in a wide adoption of the ray tracing paradigm for graphics-related tasks that spread beyond typical image synthesis. In this work, we propose an alternative formulation of the radius search operation that maps the problem to the ray tracing paradigm, in order to take advantage of the available GPU-accelerated solutions for it. We demonstrate the performance gain relative to traditional spatial search methods, especially on dynamically updated sample sets, using two representative applications: geometry processing point-wise operations on scanned point clouds and global illumination via progressive photon mapping.},
  file = {/Users/julianstamm/Zotero/storage/LYGE7YAG/Evangelou et al. - 2021 - Fast Radius Search Exploiting Ray-Tracing Frameworks, JCGT.pdf}
}

@article{frolov2012,
  title = {Irradiance {{Cache}} for a {{GPU Ray Tracer}}},
  author = {Frolov, Vladimir and Vostryakov, Konstantin and Kharlamov, Alexander and Galaktionov, V.},
  date = {2012-09-01},
  journaltitle = {GraphiCon'2012 conference proceedings},
  shortjournal = {GraphiCon'2012 conference proceedings},
  abstract = {Figure 1. The presented screenshots where rendered at 1920x1200 resolution on a GTX 560 HW under 5 minutes using Irradiance Caching (IC) technique. We achieved from 5 to 15 times acceleration compare to our naive path tracing implementation. Abstract This work proposes a GPU friendly irradiance caching (IC) solution, where performance critical parts of an irradiance cache algorithm are done completely on the GPU. We discuss some practical problems arising in the implementation of GPU irradiance caching, and propose solutions for them. The modified algorithm for the GPU is different from a CPU implementation in 2 ways. The first distinction is a multi-pass construction of irradiance cache followed by a final rendering stage and the second distinction is to insert a large record set at once instead of one by one, as used in traditional approaches. We also consider some details to efficiently implement look-up operations on the GPU.},
  file = {/Users/julianstamm/Zotero/storage/UEB2NTVI/Frolov et al. - 2012 - Irradiance Cache for a GPU Ray Tracer.pdf}
}

@online{fujii,
  title = {A Tiny Improvement of {{Oren-Nayar}} Reflectance Model},
  author = {Fujii, Yasuhiro},
  url = {https://mimosa-pudica.net/improved-oren-nayar.html},
  urldate = {2025-06-14},
  file = {/Users/julianstamm/Zotero/storage/W8I95P44/undefined}
}

@article{gao2023,
  title = {Neural {{Global Illumination}}: {{Interactive Indirect Illumination Prediction Under Dynamic Area Lights}}},
  shorttitle = {Neural {{Global Illumination}}},
  author = {Gao, Duan and Mu, Haoyuan and Xu, Kun},
  date = {2023-12},
  journaltitle = {IEEE Transactions on Visualization and Computer Graphics},
  shortjournal = {IEEE Trans. Visual. Comput. Graphics},
  volume = {29},
  number = {12},
  pages = {5325--5341},
  issn = {1077-2626, 1941-0506, 2160-9306},
  doi = {10.1109/TVCG.2022.3209963},
  url = {https://ieeexplore.ieee.org/document/9904431/},
  urldate = {2025-08-25},
  abstract = {We propose neural global illumination, a novel method for fast rendering full global illumination in static scenes with dynamic viewpoint and area lighting. The key idea of our method is to utilize a deep rendering network to model the complex mapping from each shading point to global illumination. To efficiently learn the mapping, we propose a neural-network-friendly input representation including attributes of each shading point, viewpoint information, and a combinational lighting representation that enables high-quality fitting with a compact neural network. To synthesize high-frequency global illumination effects, we transform the low-dimension input to higher-dimension space by positional encoding and model the rendering network as a deep fully-connected network. Besides, we feed a screen-space neural buffer to our rendering network to share global information between objects in the screen-space to each shading point. We have demonstrated our neural global illumination method in rendering a wide variety of scenes exhibiting complex and all-frequency global illumination effects such as multiple-bounce glossy interreflection, color bleeding, and caustics.},
  langid = {english},
  file = {/Users/julianstamm/Zotero/storage/J5K8HVP7/Gao et al. - 2023 - Neural Global Illumination Interactive Indirect Illumination Prediction Under Dynamic Area Lights.pdf}
}

@online{gao2025,
  title = {{{NeRF}}: {{Neural Radiance Field}} in {{3D Vision}}: {{A Comprehensive Review}} ({{Updated Post-Gaussian Splatting}})},
  shorttitle = {{{NeRF}}},
  author = {Gao, Kyle and Gao, Yina and He, Hongjie and Lu, Dening and Xu, Linlin and Li, Jonathan},
  date = {2025-08-10},
  eprint = {2210.00379},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2210.00379},
  url = {http://arxiv.org/abs/2210.00379},
  urldate = {2025-08-25},
  abstract = {In March 2020, Neural Radiance Field (NeRF) revolutionized Computer Vision, allowing for implicit, neural network-based scene representation and novel view synthesis. NeRF models have found diverse applications in robotics, urban mapping, autonomous navigation, virtual reality/augmented reality, and more. In August 2023, Gaussian Splatting, a direct competitor to the NeRF-based framework, was proposed, gaining tremendous momentum and overtaking NeRF-based research in terms of interest as the dominant framework for novel view synthesis. We present a comprehensive survey of NeRF papers from the past five years (2020-2025). These include papers from the pre-Gaussian Splatting era, where NeRF dominated the field for novel view synthesis and 3D implicit and hybrid representation neural field learning. We also include works from the post-Gaussian Splatting era where NeRF and implicit/hybrid neural fields found more niche applications. Our survey is organized into architecture and application-based taxonomies in the pre-Gaussian Splatting era, as well as a categorization of active research areas for NeRF, neural field, and implicit/hybrid neural representation methods. We provide an introduction to the theory of NeRF and its training via differentiable volume rendering. We also present a benchmark comparison of the performance and speed of classical NeRF, implicit and hybrid neural representation, and neural field models, and an overview of key datasets.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/julianstamm/Zotero/storage/BFWAGIAM/Gao et al. - 2025 - NeRF Neural Radiance Field in 3D Vision A Comprehensive Review (Updated Post-Gaussian Splatting).pdf;/Users/julianstamm/Zotero/storage/Z6NN48D8/2210.html}
}

@article{gassenbauer2009,
  title = {Spatial {{Directional Radiance Caching}}},
  author = {Gassenbauer, Václav and Křivánek, Jaroslav and Bouatouch, Kadi},
  date = {2009-06},
  journaltitle = {Computer Graphics Forum},
  shortjournal = {Computer Graphics Forum},
  volume = {28},
  number = {4},
  pages = {1189--1198},
  issn = {0167-7055, 1467-8659},
  doi = {10.1111/j.1467-8659.2009.01496.x},
  url = {https://onlinelibrary.wiley.com/doi/10.1111/j.1467-8659.2009.01496.x},
  urldate = {2025-08-31},
  abstract = {Abstract             We present a new approach for accelerated global illumination computation in scenes with glossy surfaces. Our algorithm combines sparse illumination computation used in the radiance caching algorithm with BRDF importance sampling. To make this approach feasible, we extend the idea of lazy illumination evaluation, used in the caching approaches, from the spatial to the directional domain. Using importance sampling allows us to apply caching not only on low‐gloss but also on shiny materials with high‐frequency BRDFs, for which the radiance caching algorithm breaks down.},
  langid = {english},
  file = {/Users/julianstamm/Zotero/storage/JIJ7D5SY/Gassenbauer et al. - 2009 - Spatial Directional Radiance Caching.pdf}
}

@online{generalizedresampledimportancesamplingfoundationsrestiruniversitaetbonn,
  title = {Generalized resampled importance sampling: foundations of ReSTIR - Universitaet Bonn},
  shorttitle = {Generalized resampled importance sampling},
  url = {https://hbz-ulb.userservices.exlibrisgroup.com},
  urldate = {2025-08-21},
  abstract = {Generalized resampled importance sampling: foundations of ReSTIR-book},
  langid = {ngerman},
  file = {/Users/julianstamm/Zotero/storage/HGDLKBHX/hbz-ulb.userservices.exlibrisgroup.com.html}
}

@online{generatingraytracedcausticeffectsunrealengine4part12020,
  title = {Generating {{Ray-Traced Caustic Effects}} in {{Unreal Engine}} 4, {{Part}} 1},
  date = {2020-12-08T19:38:03+00:00},
  url = {https://developer.nvidia.com/blog/generating-ray-traced-caustic-effects-in-unreal-engine-4-part-1/},
  urldate = {2025-05-18},
  abstract = {Caustics are common optical phenomenon in the real world. From the sloshing sparkles by water surfaces to the curved highlights in the backlight of clear glass, they are everywhere. However…},
  langid = {american},
  organization = {NVIDIA Technical Blog}
}

@article{georgiev2012,
  title = {Light Transport Simulation with Vertex Connection and Merging},
  author = {Georgiev, Iliyan and Křivánek, Jaroslav and Davidovič, Tomáš and Slusallek, Philipp},
  date = {2012-11},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {31},
  number = {6},
  pages = {1--10},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/2366145.2366211},
  url = {https://dl.acm.org/doi/10.1145/2366145.2366211},
  urldate = {2025-05-18},
  abstract = {Developing robust light transport simulation algorithms that are capable of dealing with arbitrary input scenes remains an elusive challenge. Although efficient global illumination algorithms exist, an acceptable approximation error in a reasonable amount of time is usually only achieved for specific types of input scenes. To address this problem, we present a reformulation of photon mapping as a bidirectional path sampling technique for Monte Carlo light transport simulation. The benefit of our new formulation is twofold. First, it makes it possible, for the first time, to explain in a formal manner the relative efficiency of photon mapping and bidirectional path tracing, which have so far been considered conceptually incompatible solutions to the light transport problem. Second, it allows for a seamless integration of the two methods into a more robust combined rendering algorithm via multiple importance sampling. A progressive version of this algorithm is consistent and efficiently handles a wide variety of lighting conditions, ranging from direct illumination, diffuse and glossy inter-reflections, to specular-diffusespecular light transport. Our analysis shows that this algorithm inherits the high asymptotic performance from bidirectional path tracing for most light path types, while benefiting from the efficiency of photon mapping for specular-diffuse-specular lighting effects.},
  langid = {english},
  file = {/Users/julianstamm/Zotero/storage/MZXA4C7C/Georgiev et al. - 2012 - Light transport simulation with vertex connection and merging.pdf;/Users/julianstamm/Zotero/storage/35FI9KHH/VertexMerging.html}
}

@article{goral1984,
  title = {Modeling the Interaction of Light between Diffuse Surfaces},
  author = {Goral, Cindy M. and Torrance, Kenneth E. and Greenberg, Donald P. and Battaile, Bennett},
  date = {1984-07},
  journaltitle = {ACM SIGGRAPH Computer Graphics},
  shortjournal = {SIGGRAPH Comput. Graph.},
  volume = {18},
  number = {3},
  pages = {213--222},
  issn = {0097-8930},
  doi = {10.1145/964965.808601},
  url = {https://dl.acm.org/doi/10.1145/964965.808601},
  urldate = {2025-08-11},
  abstract = {A method is described which models the interaction of light between diffusely reflecting surfaces. Current light reflection models used in computer graphics do not account for the object-to-object reflection between diffuse surfaces, and thus incorrectly compute the global illumination effects. The new procedure, based on methods used in thermal engineering, includes the effects of diffuse light sources of finite area, as well as the “color-bleeding” effects which are caused by the diffuse reflections. A simple environment is used to illustrate these simulated effects and is presented with photographs of a physical model. The procedure is applicable to environments composed of ideal diffuse reflectors and can account for direct illumination from a variety of light sources. The resultant surface intensities are independent of observer position, and thus environments can be preprocessed for dynamic sequences.},
  langid = {english},
  file = {/Users/julianstamm/Zotero/storage/NI6YSITD/Goral et al. - 1984 - Modeling the interaction of light between diffuse surfaces.pdf}
}

@inproceedings{green2003,
  title = {Spherical {{Harmonic Lighting}}: {{The Gritty Details}}},
  shorttitle = {Spherical Harmonic Lighting},
  booktitle = {Archives of the Game Developers Conference},
  author = {Green, Robin},
  date = {2003-01-16},
  volume = {56},
  pages = {4},
  url = {https://3dvar.com/Green2003Spherical.pdf},
  urldate = {2025-08-22},
  langid = {english},
  file = {/Users/julianstamm/Zotero/storage/P8XKMNYB/Green - 2003 - Spherical harmonic lighting The gritty details.pdf}
}

@article{greger1998,
  title = {The Irradiance Volume},
  author = {Greger, Gene and Shirley, Peter and Hubbard, Philip M. and Greenberg, Donald P.},
  date = {1998-03/1998-04},
  journaltitle = {IEEE Computer Graphics and Applications},
  shortjournal = {IEEE Comput. Grap. Appl.},
  volume = {18},
  number = {2},
  pages = {32--43},
  publisher = {IEEE},
  issn = {02721716},
  doi = {10.1109/38.656788},
  url = {https://ieeexplore.ieee.org/abstract/document/656788/},
  urldate = {2025-06-27},
  file = {/Users/julianstamm/Zotero/storage/CYPD9XMP/Greger et al. - 1998 - The irradiance volume.pdf;/Users/julianstamm/Zotero/storage/JJEZB4CW/The irradiance volume.pdf;/Users/julianstamm/Zotero/storage/PWR638VQ/Greger et al. - 1998 - The irradiance volume.pdf;/Users/julianstamm/Zotero/storage/RFQNFDU2/mcg1998020032.pdf}
}

@article{gunther,
  title = {Realtime {{Caustics Using Distributed Photon Mapping}}},
  author = {Günther, Johannes and Wald, Ingo and Slusallek, Philipp},
  abstract = {With the advancements in realtime ray tracing and new global illumination algorithms we are now able to render the most important illumination effects at interactive rates. One of the major remaining issues is the fast and efficient simulation of caustic illumination, such as e.g. the illumination from a car headlight. The photon mapping algorithm is a simple and robust approach that generates high-quality results and is the preferred algorithm for computing caustic illumination. However, photon mapping has a number of properties that make it rather slow on today’s processors. Photon mapping has also been notoriously difficult to parallelize efficiently.},
  langid = {english},
  file = {/Users/julianstamm/Zotero/storage/Q88BQMKN/Günther et al. - Realtime Caustics Using Distributed Photon Mapping.pdf}
}

@article{hachisuka2008,
  title = {Progressive Photon Mapping},
  author = {Hachisuka, Toshiya and Ogaki, Shinji and Jensen, Henrik Wann},
  date = {2008-12-01},
  journaltitle = {ACM Trans. Graph.},
  volume = {27},
  number = {5},
  pages = {130:1--130:8},
  issn = {0730-0301},
  doi = {10.1145/1409060.1409083},
  url = {https://dl.acm.org/doi/10.1145/1409060.1409083},
  urldate = {2025-05-13},
  abstract = {This paper introduces a simple and robust progressive global illumination algorithm based on photon mapping. Progressive photon mapping is a multi-pass algorithm where the first pass is ray tracing followed by any number of photon tracing passes. Each photon tracing pass results in an increasingly accurate global illumination solution that can be visualized in order to provide progressive feedback. Progressive photon mapping uses a new radiance estimate that converges to the correct radiance value as more photons are used. It is not necessary to store the full photon map, and unlike standard photon mapping it possible to compute a global illumination solution with any desired accuracy using a limited amount of memory. Compared with existing Monte Carlo ray tracing methods progressive photon mapping provides an efficient and robust alternative in the presence of complex light transport such as caustics and in particular reflections of caustics.},
  file = {/Users/julianstamm/Zotero/storage/K5CVIX5T/Hachisuka et al. - 2008 - Progressive photon mapping.pdf;/Users/julianstamm/Zotero/storage/PXUM7QZ3/Hachisuka et al. - Progressive Photon Mapping.pdf}
}

@inproceedings{hachisuka2009a,
  title = {Stochastic Progressive Photon Mapping},
  booktitle = {{{ACM SIGGRAPH Asia}} 2009 Papers},
  author = {Hachisuka, Toshiya and Jensen, Henrik Wann},
  date = {2009-12-01},
  series = {{{SIGGRAPH Asia}} '09},
  pages = {1--8},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/1661412.1618487},
  url = {https://dl.acm.org/doi/10.1145/1661412.1618487},
  urldate = {2025-05-13},
  abstract = {This paper presents a simple extension of progressive photon mapping for simulating global illumination with effects such as depth-of-field, motion blur, and glossy reflections. Progressive photon mapping is a robust global illumination algorithm that can handle complex illumination settings including specular-diffuse-specular paths. The algorithm can compute the correct radiance value at a point in the limit. However, progressive photon mapping is not effective at rendering distributed ray tracing effects, such as depth-of-field, that requires multiple pixel samples in order to compute the correct average radiance value over a region. In this paper, we introduce a new formulation of progressive photon mapping, called stochastic progressive photon mapping, which makes it possible to compute the correct average radiance value for a region. The key idea is to use shared photon statistics within the region rather than isolated photon statistics at a point. The algorithm is easy to implement, and our results demonstrate how it efficiently handles scenes with distributed ray tracing effects, while maintaining the robustness of progressive photon mapping in scenes with complex lighting.},
  isbn = {978-1-60558-858-2},
  file = {/Users/julianstamm/Zotero/storage/B483Y8QF/Hachisuka and Jensen - 2009 - Stochastic progressive photon mapping.pdf}
}

@inproceedings{hachisuka2010,
  title = {Parallel Progressive Photon Mapping on {{GPUs}}},
  booktitle = {{{ACM SIGGRAPH ASIA}} 2010 {{Sketches}}},
  author = {Hachisuka, Toshiya and Jensen, Henrik Wann},
  date = {2010-12-15},
  series = {{{SA}} '10},
  pages = {1},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/1899950.1900004},
  url = {https://dl.acm.org/doi/10.1145/1899950.1900004},
  urldate = {2025-05-13},
  abstract = {Accurate global illumination rendering using GPUs is gaining attention because of the highly parallel nature of global illumination algorithms. For example, computing the radiance of each pixel using path tracing is embarrassingly parallel. Some major commercial rendering software also started adopting global illumination on GPUs.},
  isbn = {978-1-4503-0523-5},
  file = {/Users/julianstamm/Zotero/storage/CSY6HFUG/Hachisuka and Jensen - 2010 - Parallel progressive photon mapping on GPUs.pdf}
}

@article{hachisuka2012,
  title = {A Path Space Extension for Robust Light Transport Simulation},
  author = {Hachisuka, Toshiya and Pantaleoni, Jacopo and Jensen, Henrik Wann},
  date = {2012-11},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {31},
  number = {6},
  pages = {1--10},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/2366145.2366210},
  url = {https://dl.acm.org/doi/10.1145/2366145.2366210},
  urldate = {2025-08-03},
  abstract = {We present a new sampling space for light transport paths that makes it possible to describe Monte Carlo path integration and photon density estimation in the same framework. A key contribution of our paper is the introduction of vertex perturbations, which extends the space of paths with loosely coupled connections. The new framework enables the computation of path probabilities in the same space under the same measure, which allows us to use multiple importance sampling to combine Monte Carlo path integration and photon density estimation. The resulting algorithm,               unified path sampling               , can robustly render complex combinations and glossy surfaces and caustics that are problematic for existing light transport simulation methods.},
  langid = {english},
  file = {/Users/julianstamm/Zotero/storage/YVYVZYPY/Hachisuka et al. - 2012 - A path space extension for robust light transport simulation.pdf}
}

@article{hachisuka2012a,
  title = {A Path Space Extension for Robust Light Transport Simulation},
  author = {Hachisuka, Toshiya and Pantaleoni, Jacopo and Jensen, Henrik Wann},
  date = {2012-11},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {31},
  number = {6},
  pages = {1--10},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/2366145.2366210},
  url = {https://dl.acm.org/doi/10.1145/2366145.2366210},
  urldate = {2025-08-28},
  abstract = {We present a new sampling space for light transport paths that makes it possible to describe Monte Carlo path integration and photon density estimation in the same framework. A key contribution of our paper is the introduction of vertex perturbations, which extends the space of paths with loosely coupled connections. The new framework enables the computation of path probabilities in the same space under the same measure, which allows us to use multiple importance sampling to combine Monte Carlo path integration and photon density estimation. The resulting algorithm,               unified path sampling               , can robustly render complex combinations and glossy surfaces and caustics that are problematic for existing light transport simulation methods.},
  langid = {english},
  file = {/Users/julianstamm/Zotero/storage/YCTPX9V7/Hachisuka et al. - 2012 - A path space extension for robust light transport simulation.pdf}
}

@article{hadadan2021,
  title = {Neural Radiosity},
  author = {Hadadan, Saeed and Chen, Shuhong and Zwicker, Matthias},
  date = {2021-12},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {40},
  number = {6},
  pages = {1--11},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/3478513.3480569},
  url = {https://dl.acm.org/doi/10.1145/3478513.3480569},
  urldate = {2025-08-25},
  abstract = {We introduce Neural Radiosity, an algorithm to solve the rendering equation by minimizing the norm of its residual, similar as in classical radiosity techniques. Traditional basis functions used in radiosity, such as piecewise polynomials or meshless basis functions are typically limited to representing isotropic scattering from diffuse surfaces. Instead, we propose to leverage neural networks to represent the full four-dimensional radiance distribution, directly optimizing network parameters to minimize the norm of the residual. Our approach decouples solving the rendering equation from rendering (perspective) images similar as in traditional radiosity techniques, and allows us to efficiently synthesize arbitrary views of a scene. In addition, we propose a network architecture using geometric learnable features that improves convergence of our solver compared to previous techniques. Our approach leads to an algorithm that is simple to implement, and we demonstrate its effectiveness on a variety of scenes with diffuse and non-diffuse surfaces.},
  langid = {english},
  file = {/Users/julianstamm/Zotero/storage/TIJTGEPW/Hadadan et al. - 2021 - Neural radiosity.pdf;/Users/julianstamm/Zotero/storage/ZKN7QNNP/3478513.html}
}

@inproceedings{halen2021,
  title = {Global {{Illumination}} Based on {{Surfels}}},
  booktitle = {{{ACM SIGGRAPH}} 2021 {{Advances}} in {{Real-Time Rendering}} in {{Game Course}}},
  author = {Halén, Henrik and Hayward, Kyle and Brinck, Andreas and Bei, Xiangshun},
  date = {2021},
  url = {https://advances.realtimerendering.com/s2021/index.html},
  urldate = {2025-08-23},
  eventtitle = {{{SIGGRAPH}} 2021},
  file = {/Users/julianstamm/Zotero/storage/8CWE8US7/advances.realtimerendering.coms2021SIGGRAPH Advances 2021 - Surfel GI.pdf.pdf}
}

@article{heckbert1990,
  title = {Adaptive Radiosity Textures for Bidirectional Ray Tracing},
  author = {Heckbert, Paul S.},
  date = {1990-09},
  journaltitle = {ACM SIGGRAPH Computer Graphics},
  shortjournal = {SIGGRAPH Comput. Graph.},
  volume = {24},
  number = {4},
  pages = {145--154},
  issn = {0097-8930},
  doi = {10.1145/97880.97895},
  url = {https://dl.acm.org/doi/10.1145/97880.97895},
  urldate = {2025-04-24},
  abstract = {We present a rendering method designed to provide accurate, general simulation of global illumination for realistic image synthesis. Separating surface interaction into diffuse plus specular, we compute the specular component on the fly, as in ray tracing, and store the diffuse component (the radiosity) for later-reuse, similar to a radiosity algorithm. Radiosities are stored in               adaptive radiosity textures (rexes               )               1               that record the pattern of light and shadow on every diffuse surface in the scene. They adaptively subdivide themselves to the appropriate level of detail for the picture being made, resolving sharp shadow edges automatically.We use a three-pass, bidirectional ray tracing algorithm that traces rays from both the lights and the eye. The "size pass" records visibility information on diffuse surfaces; the "light pass" progressively traces rays from lights and bright surfaces to deposit photons on diffuse surfaces to construct the radiosity textures; and the "eye pass" traces rays from the eye, collecting light from diffuse surfaces to make a picture.},
  langid = {english},
  file = {/Users/julianstamm/Zotero/storage/9LMSPYEB/Heckbert - 1990 - Adaptive radiosity textures for bidirectional ray tracing.pdf}
}

@article{hedstrom2025,
  title = {{{ReSTIR BDPT}}: {{Bidirectional ReSTIR Path Tracing}} with {{Caustics}}},
  shorttitle = {{{ReSTIR BDPT}}},
  author = {Hedstrom, Trevor and Kettunen, Markus and Lin, Daqi and Wyman, Chris and Li, Tzu-Mao},
  date = {2025-06-17},
  journaltitle = {ACM Trans. Graph.},
  issn = {0730-0301},
  doi = {10.1145/3744898},
  url = {https://dl.acm.org/doi/10.1145/3744898},
  urldate = {2025-07-10},
  abstract = {Recent spatiotemporal resampling algorithms (ReSTIR) accelerate real-time path tracing by reusing samples between pixels and frames. However, existing methods are limited by the sampling quality of path tracing, making them inefficient for scenes with caustics and hard-to-reach lights. We develop a ReSTIR variant incorporating bidirectional path tracing that significantly improves the sampling quality in these scenes. Combining bidirectional path tracing and ReSTIR introduces multiple challenges: the generalized resampled importance sampling (GRIS) behind ReSTIR is, by default, not aware of how a path was sampled, which complicates reuse of bidirectional paths. Light tracing is also challenging since light subpaths can contribute to all pixels. To address these challenges, we apply GRIS in a sampling technique-aware extended path space, design a bidirectional hybrid shift mapping, and introduce caustics reservoirs that can accumulate caustics across frames. Our method takes around 50ms per frame across our test scenes, and achieves significantly lower error compared to prior unidirectional ReSTIR variants running in equal time.},
  annotation = {Just Accepted},
  file = {/Users/julianstamm/Zotero/storage/3ASKHNZD/Hedstrom et al. - 2025 - ReSTIR BDPT Bidirectional ReSTIR Path Tracing with Caustics.pdf}
}

@article{heinrich1994,
  title = {Quasi-{{Monte Carlo Methods}} in {{Computer Graphics Part II}}: {{The Radiance Equation}}},
  shorttitle = {Quasi-{{Monte Carlo Methods}} in {{Computer Graphics Part II}}},
  author = {Heinrich, Stefan and Keller, Alexander},
  date = {1994},
  journaltitle = {Interner Bericht},
  volume = {243},
  pages = {94},
  publisher = {Citeseer},
  url = {https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=c7420a077ac9ffc31b22f07366132432df9ae046},
  urldate = {2025-06-21}
}

@report{heinrich1994a,
  title = {Quasi-{{Monte Carlo Methods}} in {{Computer Graphics}}, {{Part I}}: {{The QMC-Bu}} Er},
  shorttitle = {Quasi-{{Monte Carlo Methods}} in {{Computer Graphics}}, {{Part I}}},
  author = {Heinrich, Stefan and Keller, Alexander},
  date = {1994},
  institution = {Citeseer},
  url = {https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=f9a420d0c5739e48b82f4eb7ab9e017f4e7eb658},
  urldate = {2025-06-21}
}

@article{heitz2014,
  title = {Importance {{Sampling Microfacet}}‐{{Based BSDFs}} Using the {{Distribution}} of {{Visible Normals}}},
  author = {Heitz, Eric and family=Eon, given=E., prefix=d', useprefix=true},
  date = {2014-07},
  journaltitle = {Computer Graphics Forum},
  shortjournal = {Computer Graphics Forum},
  volume = {33},
  number = {4},
  pages = {103--112},
  issn = {0167-7055, 1467-8659},
  doi = {10.1111/cgf.12417},
  url = {https://onlinelibrary.wiley.com/doi/10.1111/cgf.12417},
  urldate = {2025-05-23},
  abstract = {Abstract             We present a new approach to microfacet‐based BSDF importance sampling. Previously proposed sampling schemes for popular analytic BSDFs typically begin by choosing a microfacet normal at random in a way that is independent of direction of incident light. To sample the full BSDF using these normals requires arbitrarily large sample weights leading to possible fireflies. Additionally, at grazing angles nearly half of the sampled normals face away from the incident ray and must be rejected, making the sampling scheme inefficient. Instead, we show how to use the distribution of visible normals directly to generate samples, where normals are weighted by their projection factor toward the incident direction. In this way, no backfacing normals are sampled and the sample weights contain only the shadowing factor of outgoing rays (and additionally a Fresnel term for conductors). Arbitrarily large sample weights are avoided and variance is reduced. Since the BSDF depends on the microsurface model, we describe our sampling algorithm for two models: the V‐cavity and the Smith models. We demonstrate results for both isotropic and anisotropic rough conductors and dielectrics with Beckmann and GGX distributions.},
  langid = {english},
  file = {/Users/julianstamm/Zotero/storage/CIY8DU66/Heitz and d'Eon - 2014 - Importance Sampling Microfacet‐Based BSDFs using the Distribution of Visible Normals.pdf}
}

@article{heitz2014a,
  title = {Understanding the {{Masking-Shadowing Function}} in {{Microfacet-Based BRDFs}}},
  author = {Heitz, Eric},
  date = {2014},
  journaltitle = {Journal of Computer Graphics Techniques},
  volume = {3},
  number = {2},
  pages = {32--91},
  url = {https://inria.hal.science/hal-01024289/},
  urldate = {2025-08-21},
  langid = {english},
  file = {/Users/julianstamm/Zotero/storage/4XF7GXY4/Heitz - 2014 - Understanding the Masking-Shadowing Function in Microfacet-Based BRDFs.pdf;/Users/julianstamm/Zotero/storage/PUJRRPTQ/Heitz - 2014 - Understanding the masking-shadowing function in microfacet-based BRDFs.pdf}
}

@article{heitz2018,
  title = {Sampling the {{GGX Distribution}} of {{Visible Normals}}},
  author = {Heitz, Eric},
  date = {2018},
  journaltitle = {Journal of Computer Graphics Techniques},
  shortjournal = {JCGT},
  volume = {7},
  number = {4},
  abstract = {Importance sampling microfacet bidirectional scattering distribution functions (BSDFs) using their distribution of visible normals (VNDF) yields significant variance reduction in Monte Carlo rendering. In this article, we describe an efficient and exact sampling routine for the VNDF of the GGX microfacet distribution. This routine leverages the property that GGX is the distribution of normals of a truncated ellipsoid, and sampling the GGX VNDF is equivalent to sampling the 2D projection of this truncated ellipsoid. To do that, we simplify the problem by using the linear transformation that maps the truncated ellipsoid to a hemisphere. Since linear transformations preserve the uniformity of projected areas, sampling in the hemisphere configuration and transforming the samples back to the ellipsoid configuration yields valid samples from the GGX VNDF.},
  langid = {english},
  file = {/Users/julianstamm/Zotero/storage/QC7SR6WD/Heitz - 2018 - Sampling the GGX Distribution of Visible Normals.pdf}
}

@online{hermosilla2019,
  title = {Deep-Learning the {{Latent Space}} of {{Light Transport}}},
  author = {Hermosilla, Pedro and Maisch, Sebastian and Ritschel, Tobias and Ropinski, Timo},
  date = {2019-06-30},
  eprint = {1811.04756},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1811.04756},
  url = {http://arxiv.org/abs/1811.04756},
  urldate = {2025-08-25},
  abstract = {We suggest a method to directly deep-learn light transport, i. e., the mapping from a 3D geometry-illumination-material configuration to a shaded 2D image. While many previous learning methods have employed 2D convolutional neural networks applied to images, we show for the first time that light transport can be learned directly in 3D. The benefit of 3D over 2D is, that the former can also correctly capture illumination effects related to occluded and/or semi-transparent geometry. To learn 3D light transport, we represent the 3D scene as an unstructured 3D point cloud, which is later, during rendering, projected to the 2D output image. Thus, we suggest a two-stage operator comprising of a 3D network that first transforms the point cloud into a latent representation, which is later on projected to the 2D output image using a dedicated 3D-2D network in a second step. We will show that our approach results in improved quality in terms of temporal coherence while retaining most of the computational efficiency of common 2D methods. As a consequence, the proposed two stage-operator serves as a valuable extension to modern deferred shading approaches.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  file = {/Users/julianstamm/Zotero/storage/SJKCJLYB/Hermosilla et al. - 2019 - Deep-learning the Latent Space of Light Transport.pdf;/Users/julianstamm/Zotero/storage/ND5T5UZF/1811.html}
}

@article{hery2013,
  title = {Physically Based Lighting at Pixar},
  author = {Hery, Christophe and Villemin, Ryusuke and Studios, Pixar Animation},
  date = {2013},
  journaltitle = {Physically Based Shading’SIGGRAPH Course},
  publisher = {Citeseer},
  url = {https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=0c614421eeb49d910613b75641b766a96f6a9e32},
  urldate = {2025-06-21},
  file = {/Users/julianstamm/Zotero/storage/57836DJR/Hery et al. - 2013 - Physically based lighting at pixar.pdf}
}

@article{hoetzlein,
  title = {Interactive {{Million-Particle Fluids}}},
  author = {Hoetzlein, Rama C and Devtech, Graphics},
  langid = {english},
  file = {/Users/julianstamm/Zotero/storage/8L9DW5LI/Hoetzlein and Devtech - INTERACTIVE MILLION-PARTICLE FLUIDS.pdf}
}

@article{hooker2016,
  title = {Volumetric Global Illumination at {{Treyarch}}},
  author = {Hooker, John T.},
  date = {2016},
  journaltitle = {Advances in Real-Time Rendering},
  url = {https://www.activision.com/cdn/research/Volumetric_Global_Illumination_at_Treyarch.pdf},
  urldate = {2025-08-23},
  file = {/Users/julianstamm/Zotero/storage/BTB6QHF9/Hooker - 2016 - Volumetric global illumination at Treyarch.pdf}
}

@software{horizonresearchrtnn2025,
  title = {Horizon-Research/Rtnn},
  date = {2025-05-19T01:31:53Z},
  origdate = {2021-06-18T00:32:22Z},
  url = {https://github.com/horizon-research/rtnn},
  urldate = {2025-05-27},
  organization = {horizon-research}
}

@article{hu2021,
  title = {Efficient Real-Time Dynamic Diffuse Global Illumination Using Signed Distance Fields},
  author = {Hu, Jinkai and Yip, Milo K. and Alonso, Guillermo Elias and Gu, Shihao and Tang, Xiangjun and Jin, Xiaogang},
  date = {2021-09},
  journaltitle = {The Visual Computer},
  shortjournal = {Vis Comput},
  volume = {37},
  number = {9--11},
  pages = {2539--2551},
  issn = {0178-2789, 1432-2315},
  doi = {10.1007/s00371-021-02197-0},
  url = {https://link.springer.com/10.1007/s00371-021-02197-0},
  urldate = {2025-08-22},
  langid = {english}
}

@article{immel1986,
  title = {A Radiosity Method for Non-Diffuse Environments},
  author = {Immel, David S. and Cohen, Michael F. and Greenberg, Donald P.},
  date = {1986-08-31},
  journaltitle = {ACM SIGGRAPH Computer Graphics},
  shortjournal = {SIGGRAPH Comput. Graph.},
  volume = {20},
  number = {4},
  pages = {133--142},
  issn = {0097-8930},
  doi = {10.1145/15886.15901},
  url = {https://dl.acm.org/doi/10.1145/15886.15901},
  urldate = {2025-08-21},
  abstract = {A general radiosity method accounting for all interreflections of light between diffuse and nondiffuse surfaces in complex environments is introduced. As contrasted with previous radiosity methods, surfaces are no longer required to be perfectly diffuse reflectors and emitters. A complete, viewer independent description of the light leaving each surface in each direction is computed, allowing dynamic sequences of images to be rendered with little additional computation per image. Phenomena such as "reflection tracking", reflections following a moving observer across a specular surface are produced. Secondary light sources, such as the light from a spotlight reflecting off a mirror onto a wall are also accounted for.},
  langid = {english},
  file = {/Users/julianstamm/Zotero/storage/DKFHEDA6/Immel et al. - 1986 - A radiosity method for non-diffuse environments.pdf}
}

@online{improveshaderperformanceingameframeratesshaderexecutionreordering2022,
  title = {Improve {{Shader Performance}} and {{In-Game Frame Rates}} with {{Shader Execution Reordering}}},
  date = {2022-10-13T00:01:30+00:00},
  url = {https://developer.nvidia.com/blog/improve-shader-performance-and-in-game-frame-rates-with-shader-execution-reordering/},
  urldate = {2025-05-18},
  abstract = {Learn about Shader Execution Reordering (SER), a performance optimization that unlocks the potential for better ray and memory coherency in ray tracing shaders.},
  langid = {american},
  organization = {NVIDIA Technical Blog}
}

@article{iwanicki2017,
  title = {Precomputed Lighting in {{Call}} of {{Duty}}: {{Infinite Warfare}}},
  shorttitle = {Precomputed Lighting in {{Call}} of {{Duty}}},
  author = {Iwanicki, Michał and Sloan, Peter-Pike},
  date = {2017},
  journaltitle = {Advances in Real-Time Rendering in Games, Part I (ACM SIGGRAPH Courses)},
  url = {https://advances.realtimerendering.com/s2017/},
  file = {/Users/julianstamm/Zotero/storage/7LV7B6XY/Precomputed Lighting in CoD IW_20_PPS.pdf}
}

@inproceedings{iwanicki2024,
  title = {The {{Neural Light Grid}}: {{A Scalable Production-Ready Learned Irradiance Volume}}},
  booktitle = {{{ACM SIGGRAPH}} 2024 {{Courses}}: {{Advances}} in {{Real-Time Rendering}} in {{Games}}},
  author = {Iwanicki, Michał and Sloan, Peter-Pike and Silvennoinen, Ari and Shirley, Peter},
  date = {2024-07-30},
  publisher = {Activision Research},
  eventtitle = {{{SIGGRAPH}} 2024},
  langid = {english},
  file = {/Users/julianstamm/Zotero/storage/Y8HMVLF3/Iwanicki et al. - The Neural Light Grid A Scalable Production-Ready Learned Irradiance Volume.pdf}
}

@article{jakob2012,
  title = {Manifold Exploration: A {{Markov Chain Monte Carlo}} Technique for Rendering Scenes with Difficult Specular Transport},
  shorttitle = {Manifold Exploration},
  author = {Jakob, Wenzel and Marschner, Steve},
  date = {2012-08-05},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {31},
  number = {4},
  pages = {1--13},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/2185520.2185554},
  url = {https://dl.acm.org/doi/10.1145/2185520.2185554},
  urldate = {2025-08-21},
  abstract = {It is a long-standing problem in unbiased Monte Carlo methods for rendering that certain difficult types of light transport paths, particularly those involving viewing and illumination along paths containing specular or glossy surfaces, cause unusably slow convergence. In this paper we introduce Manifold Exploration, a new way of handling specular paths in rendering. It is based on the idea that sets of paths contributing to the image naturally form manifolds in path space, which can be explored locally by a simple equation-solving iteration. This paper shows how to formulate and solve the required equations using only geometric information that is already generally available in ray tracing systems, and how to use this method in in two different Markov Chain Monte Carlo frameworks to accurately compute illumination from general families of paths. The resulting rendering algorithms handle specular, near-specular, glossy, and diffuse surface interactions as well as isotropic or highly anisotropic volume scattering interactions, all using the same fundamental algorithm. An implementation is demonstrated on a range of challenging scenes and evaluated against previous methods.},
  langid = {english},
  file = {/Users/julianstamm/Zotero/storage/QWKR45P7/Jakob and Marschner - 2012 - Manifold exploration a Markov Chain Monte Carlo technique for rendering scenes with difficult specu.pdf}
}

@inproceedings{jarosz2008,
  title = {The Beam Radiance Estimate for Volumetric Photon Mapping},
  booktitle = {{{ACM SIGGRAPH}} 2008 Classes},
  author = {Jarosz, Wojciech and Zwicker, Matthias and Jensen, Henrik Wann},
  date = {2008-08-11},
  pages = {1--112},
  publisher = {ACM},
  location = {Los Angeles California},
  doi = {10.1145/1401132.1401137},
  url = {https://dl.acm.org/doi/10.1145/1401132.1401137},
  urldate = {2025-07-12},
  eventtitle = {{{SIGGRAPH}} '08: {{Special Interest Group}} on {{Computer Graphics}} and {{Interactive Techniques Conference}}},
  file = {/Users/julianstamm/Zotero/storage/9CVC9P4Z/Jarosz et al. - 2008 - The beam radiance estimate for volumetric photon mapping.pdf}
}

@incollection{jensen1995,
  title = {Importance {{Driven Path Tracing}} Using the {{Photon Map}}},
  booktitle = {Rendering {{Techniques}} ’95},
  author = {Jensen, Henrik Wann},
  editor = {Hanrahan, Patrick M. and Purgathofer, Werner},
  date = {1995},
  pages = {326--335},
  publisher = {Springer Vienna},
  location = {Vienna},
  doi = {10.1007/978-3-7091-9430-0_31},
  url = {http://link.springer.com/10.1007/978-3-7091-9430-0_31},
  urldate = {2025-06-25},
  isbn = {978-3-211-82733-8 978-3-7091-9430-0},
  langid = {english},
  file = {/Users/julianstamm/Zotero/storage/GK4DF68I/Jensen - 1995 - Importance Driven Path Tracing using the Photon Map.pdf}
}

@incollection{jensen1996,
  title = {Global {{Illumination}} Using {{Photon Maps}}},
  booktitle = {Rendering {{Techniques}} ’96},
  author = {Jensen, Henrik Wann},
  editor = {Pueyo, Xavier and Schröder, Peter},
  date = {1996},
  pages = {21--30},
  publisher = {Springer Vienna},
  location = {Vienna},
  doi = {10.1007/978-3-7091-7484-5_3},
  url = {http://link.springer.com/10.1007/978-3-7091-7484-5_3},
  urldate = {2025-04-22},
  abstract = {This paper presents a two pass global illumination method based on the concept of photon maps. It represents a significant improvement of a previously described approach both with respect to speed, accuracy and versatility. In the first pass two photon maps are created by emitting packets of energy (photons) from the light sources and storing these as they hit surfaces within the scene. We use one high resolution caustics photon map to render caustics that are visualized directly and one low resolution photon map that is used during the rendering step. The scene is rendered using a distribution ray tracing algorithm optimized by using the information in the photon maps. Shadow photons are used to render shadows more efficiently and the directional information in the photon map is used to generate optimized sampling directions and to limit the recursion in the distribution ray tracer by providing an estimate of the radiance on all surfaces with the exception of specular and highly glossy surfaces.},
  isbn = {978-3-211-82883-0 978-3-7091-7484-5},
  langid = {english},
  file = {/Users/julianstamm/Zotero/storage/YK6TW69L/Jensen - 1996 - Global Illumination using Photon Maps.pdf}
}

@book{jensen2010,
  title = {Realistic Image Synthesis Using Photon Mapping},
  author = {Jensen, Henrik Wann},
  date = {2010},
  edition = {Nachdr.},
  publisher = {Peters},
  location = {Wellesley, Mass},
  isbn = {978-1-56881-147-5 978-1-56881-462-9},
  langid = {english},
  pagetotal = {181},
  file = {/Users/julianstamm/Zotero/storage/5J77UPC8/Jensen - 2010 - Realistic image synthesis using photon mapping.pdf}
}

@online{jiang2020,
  title = {Deep {{Radiance Caching}}: {{Convolutional Autoencoders Deeper}} in {{Ray Tracing}}},
  shorttitle = {Deep {{Radiance Caching}}},
  author = {Jiang, Giulio and Kainz, Bernhard},
  date = {2020-07-30},
  eprint = {1910.02480},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1910.02480},
  url = {http://arxiv.org/abs/1910.02480},
  urldate = {2025-08-25},
  abstract = {Rendering realistic images with global illumination is a computationally demanding task and often requires dedicated hardware for feasible runtime. Recent research uses Deep Neural Networks to predict indirect lighting on image level, but such methods are commonly limited to diffuse materials and require training on each scene.We present Deep Radiance Caching (DRC), an efficient variant of Radiance Caching utilizing Convolutional Autoencoders for rendering global illumination. DRC employs a denoising neural network with Radiance Caching to support a wide range of material types, without the requirement of offline pre-computation or training for each scene.This offers high performance CPU rendering for maximum accessibility. Our method has been evaluated on interior scenes, and is able to produce high-quality images within 180 seconds on a single CPU.},
  pubstate = {prepublished},
  keywords = {Computer Science - Graphics},
  file = {/Users/julianstamm/Zotero/storage/AR35JQIE/Jiang and Kainz - 2020 - Deep Radiance Caching Convolutional Autoencoders Deeper in Ray Tracing.pdf;/Users/julianstamm/Zotero/storage/AYQ3V9BB/1910.html;/Users/julianstamm/Zotero/storage/TDHCSGAX/S0097849320301412.html}
}

@article{jones2014,
  title = {Irradiance {{Caching}} for {{Global Illumination Calculation}} on {{Graphics Hardware}}},
  author = {Jones, Nathaniel L and Reinhart, Christoph F},
  date = {2014},
  abstract = {Recent developments in integrated circuit technology tend toward increased numbers of cores rather than faster clock speeds, so software must use parallelism to achieve faster run times. The ray tracing performed by Radiance is highly parallelizable in concept, with the exception of irradiance caching that serially stores and retrieves results of expensive indirect irradiation computations. This paper describes a novel method of parallel irradiance caching for global illumination on a graphics processing unit (GPU).},
  langid = {english},
  file = {/Users/julianstamm/Zotero/storage/3WP8WKRP/Jones and Reinhart - 2014 - IRRADIANCE CACHING FOR GLOBAL ILLUMINATION CALCULATION ON GRAPHICS HARDWARE.pdf}
}

@inproceedings{kajiya1986,
  title = {The Rendering Equation},
  booktitle = {Proceedings of the 13th Annual Conference on {{Computer}} Graphics and Interactive Techniques},
  author = {Kajiya, James T.},
  date = {1986-08-31},
  series = {{{SIGGRAPH}} '86},
  pages = {143--150},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/15922.15902},
  url = {https://dl.acm.org/doi/10.1145/15922.15902},
  urldate = {2024-02-23},
  abstract = {We present an integral equation which generalizes a variety of known rendering algorithms. In the course of discussing a monte carlo solution we also present a new form of variance reduction, called Hierarchical sampling and give a number of elaborations shows that it may be an efficient new technique for a wide variety of monte carlo procedures. The resulting rendering algorithm extends the range of optical phenomena which can be effectively simulated.},
  isbn = {978-0-89791-196-2},
  keywords = {Important},
  file = {/Users/julianstamm/Zotero/storage/WZRDDU9X/Kajiya - 1986 - The rendering equation.pdf}
}

@article{kang2016,
  title = {A Survey of Photon Mapping State-of-the-Art Research and Future Challenges},
  author = {Kang, Chun-meng and Wang, Lu and Xu, Yan-ning and Meng, Xiang-xu},
  date = {2016-03-01},
  journaltitle = {Frontiers of Information Technology \& Electronic Engineering},
  shortjournal = {Frontiers Inf Technol Electronic Eng},
  volume = {17},
  number = {3},
  pages = {185--199},
  issn = {2095-9230},
  doi = {10.1631/FITEE.1500251},
  url = {https://doi.org/10.1631/FITEE.1500251},
  urldate = {2025-05-18},
  abstract = {Global illumination is the core part of photo-realistic rendering. The photon mapping algorithm is an effective method for computing global illumination with its obvious advantage of caustic and color bleeding rendering. It is an active research field that has been developed over the past two decades. The deficiency of precise details and efficient rendering are still the main challenges of photon mapping. This report reviews recent work and classifies it into a set of categories including radiance estimation, photon relaxation, photon tracing, progressive photon mapping, and parallel methods. The goals of our report are giving readers an overall introduction to photon mapping and motivating further research to address the limitations of existing methods.},
  langid = {english},
  keywords = {3-D Image Reconstruction,Applied Optics,Bioluminescence Imaging,Brain Mapping,Fluorescence Imaging,Global illumination,Multiphoton microscopy,Photon mapping,Photon relaxation,Progressive photon mapping,Radiance estimation,TP37},
  file = {/Users/julianstamm/Zotero/storage/Q9FH8HRW/Kang et al. - 2016 - A survey of photon mapping state-of-the-art research and future challenges.pdf}
}

@article{kaplanyan2013,
  title = {Adaptive Progressive Photon Mapping},
  author = {Kaplanyan, Anton S. and Dachsbacher, Carsten},
  date = {2013-04-30},
  journaltitle = {ACM Trans. Graph.},
  volume = {32},
  number = {2},
  pages = {16:1--16:13},
  issn = {0730-0301},
  doi = {10.1145/2451236.2451242},
  url = {https://dl.acm.org/doi/10.1145/2451236.2451242},
  urldate = {2025-05-18},
  abstract = {This article introduces a novel locally adaptive progressive photon mapping technique which optimally balances noise and bias in rendered images to minimize the overall error. It is the result of an analysis of the radiance estimation in progressive photon mapping. As a first step, we establish a connection to the field of recursive estimation and regression in statistics and derive the optimal estimation parameters for the asymptotic convergence of existing approaches. Next, we show how to reformulate photon mapping as a spatial regression in the measurement equation of light transport. This reformulation allows us to derive a novel data-driven bandwidth selection technique for estimating a pixel's measurement. The proposed technique possesses attractive convergence properties with finite numbers of samples, which is important for progressive rendering, and it also provides better results for quasi-converged images. Our results show the practical benefits of using our adaptive method.},
  file = {/Users/julianstamm/Zotero/storage/J7LKN8V3/Kaplanyan and Dachsbacher - 2013 - Adaptive progressive photon mapping.pdf}
}

@article{kautz2002,
  title = {Fast Arbitrary Brdf Shading for Low-Frequency Lighting Using Spherical Harmonics.},
  author = {Kautz, Jan and Snyder, John and Sloan, Peter-Pike J.},
  date = {2002},
  journaltitle = {Rendering Techniques},
  volume = {2},
  number = {291--296},
  pages = {1},
  url = {https://www.ppsloan.org/publications/shbrdf_final17.pdf},
  urldate = {2025-08-22},
  file = {/Users/julianstamm/Zotero/storage/5I5JKD9E/Kautz et al. - 2002 - Fast arbitrary brdf shading for low-frequency lighting using spherical harmonics..pdf}
}

@incollection{keller1995,
  title = {A {{Quasi-Monte Carlo Algorithm}} for the {{Global Illumination Problem}} in the {{Radiosity Setting}}},
  booktitle = {Monte {{Carlo}} and {{Quasi-Monte Carlo Methods}} in {{Scientific Computing}}},
  author = {Keller, Alexander},
  editor = {Niederreiter, Harald and Shiue, Peter Jau-Shyong},
  editora = {Bickel, P. and Diggle, P. and Fienberg, S. and Krickeberg, K. and Olkin, I. and Wermuth, N. and Zeger, S.},
  editoratype = {redactor},
  date = {1995},
  volume = {106},
  pages = {239--251},
  publisher = {Springer New York},
  location = {New York, NY},
  doi = {10.1007/978-1-4612-2552-2_15},
  url = {http://link.springer.com/10.1007/978-1-4612-2552-2_15},
  urldate = {2025-06-21},
  isbn = {978-0-387-94577-4 978-1-4612-2552-2},
  file = {/Users/julianstamm/Zotero/storage/ZFT5XLAD/Keller - 1995 - A Quasi-Monte Carlo Algorithm for the Global Illumination Problem in the Radiosity Setting.pdf}
}

@article{keller1996,
  title = {Quasi-{{Monte Carlo}} Methods in Computer Graphics: The Global Illumination Problem},
  shorttitle = {Quasi-{{Monte Carlo}} Methods in Computer Graphics},
  author = {Keller, Alexander},
  date = {1996},
  journaltitle = {LECTURES IN APPLIED MATHEMATICS-AMERICAN MATHEMATICAL SOCIETY},
  volume = {32},
  pages = {455--470},
  publisher = {Citeseer},
  url = {https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=579bca8c938f1e0f25474a02a4ff93f5b6ea8886},
  urldate = {2025-06-21},
  file = {/Users/julianstamm/Zotero/storage/T3L6VFAD/Keller - 1996 - Quasi-Monte Carlo methods in computer graphics the global illumination problem.pdf}
}

@incollection{keller1996a,
  title = {Quasi-{{Monte Carlo Radiosity}}},
  booktitle = {Rendering {{Techniques}} ’96},
  author = {Keller, Alexander},
  editor = {Pueyo, Xavier and Schröder, Peter},
  date = {1996},
  pages = {101--110},
  publisher = {Springer Vienna},
  location = {Vienna},
  doi = {10.1007/978-3-7091-7484-5_11},
  url = {http://link.springer.com/10.1007/978-3-7091-7484-5_11},
  urldate = {2025-06-21},
  isbn = {978-3-211-82883-0 978-3-7091-7484-5},
  langid = {english},
  file = {/Users/julianstamm/Zotero/storage/4K86PPHB/Keller - 1996 - Quasi-Monte Carlo Radiosity.pdf}
}

@inproceedings{keller1997,
  title = {Instant Radiosity},
  booktitle = {Proceedings of the 24th Annual Conference on {{Computer}} Graphics and Interactive Techniques  - {{SIGGRAPH}} '97},
  author = {Keller, Alexander},
  date = {1997},
  pages = {49--56},
  publisher = {ACM Press},
  location = {Not Known},
  doi = {10.1145/258734.258769},
  url = {http://portal.acm.org/citation.cfm?doid=258734.258769},
  urldate = {2025-08-25},
  eventtitle = {The 24th Annual Conference},
  isbn = {978-0-89791-896-1},
  langid = {english},
  file = {/Users/julianstamm/Zotero/storage/2JE6L7P9/Keller - 1997 - Instant radiosity.pdf}
}

@inproceedings{keller2016,
  title = {Path {{Space Filtering}}},
  booktitle = {Monte {{Carlo}} and {{Quasi-Monte Carlo Methods}}},
  author = {Keller, Alexander and Dahm, Ken and Binder, Nikolaus},
  editor = {Cools, Ronald and Nuyens, Dirk},
  date = {2016},
  volume = {163},
  pages = {423--436},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-33507-0_21},
  url = {http://link.springer.com/10.1007/978-3-319-33507-0_21},
  abstract = {We improve the efficiencyKeller, Alexanderof quasi-MonteDahm, KenCarlo integro-approximationBinder, Nikolausby using weighted averages of samples instead of the samples themselves. The proposed deterministic algorithm is constructed such that it converges to the solution of the given integro-approximation problem. The improvements and wide applicability of the consistent method are demonstrated by visual evidence in the setting of light transport simulation for photorealistic image synthesis, where the weighted averages correspond to locally smoothed contributions of path space samples.},
  isbn = {978-3-319-33507-0},
  langid = {english},
  keywords = {Integro-approximation,Photorealistic image synthesis,Rendering,Transport simulation},
  file = {/Users/julianstamm/Zotero/storage/S95UV3UP/Keller et al. - 2016 - Path Space Filtering.pdf}
}

@article{kern2023,
  title = {Accelerating {{Photon Mapping}} for {{Hardware-Based Ray Tracing}}},
  author = {Kern, René and Brüll, Felix and Grosch, Thorsten},
  date = {2023},
  journaltitle = {Journal of Computer Graphics Techniques Vol},
  volume = {12},
  number = {1},
  url = {https://jcgt.org/published/0012/01/01/paper-lowres.pdf},
  urldate = {2025-06-21},
  file = {/Users/julianstamm/Zotero/storage/4CSJ3MD4/Kern et al. - 2023 - Accelerating Photon Mapping for Hardware-Based Ray Tracing.pdf;/Users/julianstamm/Zotero/storage/BMFWUVG5/01.html}
}

@book{kern2024,
  title = {{{ReSTIR FG}}: {{Real-Time Reservoir Resampled Photon Final Gathering}}},
  shorttitle = {{{ReSTIR FG}}},
  author = {Kern, René and Brüll, Felix and Grosch, Thorsten},
  date = {2024},
  publisher = {The Eurographics Association},
  issn = {1727-3463},
  url = {https://doi.org/10.2312/sr.20241155},
  urldate = {2025-05-13},
  abstract = {Achieving real-time global illumination for a given scene remains challenging, even with the advent of hardware ray tracing, due to the substantial quantity of rays required. To enhance the quality of the limited number of samples, spatial and temporal resampling can be used. The concept of resampling gained popularity with ReSTIR DI [BWP*20], enabling real-time direct illumination for scenes with millions of lights. This concept was further extended by combining it with path tracing to quickly approximate the indirect illumination (ReSTIR GI [OLK*21]) or correctly approximate global illumination (ReSTIR PT [LKB*22] and Suffix ReSTIR [KLR*23]). However, these algorithms fall short in effectively rendering caustic effects -bundles of reflected or refracted light- often associated with photon mapping. We introduce ReSTIR FG, an efficient real time indirect illumination algorithm that combines photon final gathering with the principles of ReSTIR. First, we introduce an efficient photon final gathering scheme, enabling quick consistent offline rendering. Then we combine our photon final gathering with spatiotemporal resampling to allow for real time global illumination. Our algorithm is capable of displaying multi bounce indirect illumination, as well as caustic effects, while remaining competitive in both runtime and quality when compared to the aforementioned state-of-the-art global illumination resampling techniques.},
  isbn = {978-3-03868-262-2},
  langid = {english},
  file = {/Users/julianstamm/Zotero/storage/3SNITWFA/Kern et al. - 2024 - ReSTIR FG Real-Time Reservoir Resampled Photon Final Gathering.pdf;/Users/julianstamm/Zotero/storage/U7H4ZJMG/Kern et al. - 2024 - ReSTIR FG Real-Time Reservoir Resampled Photon Final Gathering.pdf}
}

@report{kirkpatrick2025,
  title = {Web {{Content Accessibility Guidelines}} ({{WCAG}}) 2.1},
  shorttitle = {{{WCAG}}},
  author = {Kirkpatrick, Andrew and O Connor, Joshue and Campbell, Alastair and Cooper, Michael},
  date = {2025-05-06},
  institution = {World Wide Web Consortium},
  url = {https://www.w3.org/TR/WCAG21/#dfn-relative-luminance},
  urldate = {2025-06-21},
  file = {/Users/julianstamm/Zotero/storage/4KUYZ5CJ/WCAG21.html}
}

@article{knaus2011,
  title = {Progressive Photon Mapping: {{A}} Probabilistic Approach},
  shorttitle = {Progressive Photon Mapping},
  author = {Knaus, Claude and Zwicker, Matthias},
  date = {2011-05},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {30},
  number = {3},
  pages = {1--13},
  publisher = {Association for Computing Machinery (ACM)},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/1966394.1966404},
  url = {https://dl.acm.org/doi/10.1145/1966394.1966404},
  urldate = {2025-07-12},
  abstract = {In this article we present a novel formulation of progressive photon mapping. Similar to the original progressive photon mapping algorithm, our approach is capable of computing global illumination solutions without bias in the limit, and it uses only a constant amount of memory. It produces high-quality results in situations that are difficult for most other algorithms, such as scenes with realistic light fixtures where the light sources are completely enclosed by refractive material. Our new formulation is based on a probabilistic derivation. The key property of our approach is that it does not require the maintenance of local photon statistics. In addition, our derivation allows for arbitrary kernels in the radiance estimate and includes stochastic ray tracing algorithms. Finally, our approach is readily applicable to volumetric photon mapping. We compare our algorithm to previous progressive photon mapping approaches and show that we achieve the same convergence to unbiased results, even without local photon statistics.},
  langid = {english},
  file = {/Users/julianstamm/Zotero/storage/LYTTIGJN/Knaus and Zwicker - 2011 - Progressive photon mapping A probabilistic approach.pdf}
}

@article{krivanek2005,
  title = {Radiance Caching for Efficient Global Illumination Computation},
  author = {Křivánek, Jaroslav and Gautron, Pascal and Pattanaik, Sumanta and Bouatouch, Kadi},
  date = {2005},
  journaltitle = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {11},
  number = {5},
  pages = {550--561},
  publisher = {IEEE},
  url = {https://ieeexplore.ieee.org/abstract/document/1471692/},
  urldate = {2025-07-28},
  langid = {english},
  file = {/Users/julianstamm/Zotero/storage/74CLW8PF/Krivánek et al. - 2005 - Radiance caching for efficient global illumination computation.pdf}
}

@inproceedings{krivanek2007,
  title = {Practical Global Illumination with Irradiance Caching},
  booktitle = {{{ACM SIGGRAPH}} 2007 Courses},
  author = {Křivánek, Jaroslav and Gautron, Pascal and Ward, Greg and Arikan, Okan and Jensen, Henrik Wann},
  date = {2007-08-05},
  pages = {1},
  publisher = {ACM},
  location = {San Diego California},
  doi = {10.1145/1281500.1281617},
  url = {https://dl.acm.org/doi/10.1145/1281500.1281617},
  urldate = {2025-08-22},
  eventtitle = {{{SIGGRAPH07}}: {{Special Interest Group}} on {{Computer Graphics}} and {{Interactive Techniques Conference}}},
  isbn = {978-1-4503-1823-5},
  langid = {english},
  file = {/Users/julianstamm/Zotero/storage/VTIKC352/Křivánek et al. - 2007 - Practical global illumination with irradiance caching.pdf;/Users/julianstamm/Zotero/storage/YW55DNHY/1281500.html}
}

@inproceedings{kuenlin2024,
  title = {Advanced {{Graphics Summit}}: {{Raytracing}} in {{Snowdrop}}: {{An Optimized Lighting Pipeline}} for {{Consoles}}},
  booktitle = {Advanced {{Graphics Summit}}},
  author = {Kuenlin, Quentin},
  date = {2024-03-18},
  publisher = {Massive Entertainment a Ubisoft Studio},
  url = {https://uat.gdcvault.com/play/1034763/Advanced-Graphics-Summit-Raytracing-in},
  urldate = {2025-08-24},
  abstract = {This presentation provides an overview of the pipeline used for the raytraced global illumination and raytraced reflections in the Snowdrop engine. It goes into a deep dive into the probe system used to light the forward rendered objects and also...},
  eventtitle = {Game {{Developers Conference}} 2024},
  file = {/Users/julianstamm/Zotero/storage/WVP7XHAR/Advanced Graphics Summit Raytracing in Snowdrop An Optimized Lighting Pipeline for Consoles.pdf}
}

@article{lafortune1993,
  title = {Bi-Directional Path Tracing},
  author = {Lafortune, Eric P. and Willems, Yves D.},
  date = {1993},
  url = {http://masters.donntu.ru/2013/fknt/kalamitra/library/Bidirectional%20Path%20Tracing%201990.pdf},
  urldate = {2025-08-11},
  file = {/Users/julianstamm/Zotero/storage/FBGAKEAE/Lafortune and Willems - 1993 - Bi-directional path tracing.pdf}
}

@incollection{lafortune1995,
  title = {A {{5D Tree}} to {{Reduce}} the {{Variance}} of {{Monte Carlo Ray Tracing}}},
  booktitle = {Rendering {{Techniques}} ’95},
  author = {Lafortune, Eric P. and Willems, Yves D.},
  editor = {Hanrahan, Patrick M. and Purgathofer, Werner},
  date = {1995},
  pages = {11--20},
  publisher = {Springer Vienna},
  location = {Vienna},
  doi = {10.1007/978-3-7091-9430-0_2},
  url = {http://link.springer.com/10.1007/978-3-7091-9430-0_2},
  urldate = {2025-06-25},
  isbn = {978-3-211-82733-8 978-3-7091-9430-0},
  langid = {english},
  file = {/Users/julianstamm/Zotero/storage/V5FSIPBZ/Lafortune and Willems - 1995 - A 5D Tree to Reduce the Variance of Monte Carlo Ray Tracing.pdf}
}

@article{lagarde2014,
  title = {Moving {{Frostbite}} to {{Physically Based Rendering}} 3.0},
  author = {Lagarde, Sébastien and family=Rousiers, given=Charles, prefix=de, useprefix=true},
  date = {2014},
  journaltitle = {SIGGRAPH},
  langid = {english},
  file = {/Users/julianstamm/Zotero/storage/N4P276LI/s2014_pbs_frostbite_slides.pdf;/Users/julianstamm/Zotero/storage/Q5HSZPJR/Lagarde and de Rousiers - Moving Frostbite to Physically Based Rendering 3.0.pdf}
}

@online{lagarde2017,
  title = {Physically {{Based Material Where Are We}}},
  author = {Lagarde, Sébastien},
  date = {2017},
  url = {https://openproblems.realtimerendering.com/s2017/02-PhysicallyBasedMaterialWhereAreWe.pdf},
  urldate = {2025-04-29},
  file = {/Users/julianstamm/Zotero/storage/C5BT57M4/Physically Based Material Where Are We.pdf}
}

@online{lehtinen2018,
  title = {{{Noise2Noise}}: {{Learning Image Restoration}} without {{Clean Data}}},
  shorttitle = {{{Noise2Noise}}},
  author = {Lehtinen, Jaakko and Munkberg, Jacob and Hasselgren, Jon and Laine, Samuli and Karras, Tero and Aittala, Miika and Aila, Timo},
  date = {2018-10-29},
  eprint = {1803.04189},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1803.04189},
  url = {http://arxiv.org/abs/1803.04189},
  urldate = {2025-08-02},
  abstract = {We apply basic statistical reasoning to signal reconstruction by machine learning -- learning to map corrupted observations to clean signals -- with a simple and powerful conclusion: it is possible to learn to restore images by only looking at corrupted examples, at performance at and sometimes exceeding training using clean data, without explicit image priors or likelihood models of the corruption. In practice, we show that a single model learns photographic noise removal, denoising synthetic Monte Carlo images, and reconstruction of undersampled MRI scans -- all corrupted by different processes -- based on noisy data only.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/julianstamm/Zotero/storage/6HSP7GAZ/Lehtinen et al. - 2018 - Noise2Noise Learning Image Restoration without Clean Data.pdf;/Users/julianstamm/Zotero/storage/AWGE2CHZ/1803.html}
}

@article{liao2025,
  title = {A {{Survey}} on {{Neural Radiance Fields}}},
  author = {Liao, Yun and Di, Yide and Zhou, Hao and Zhu, Kaijun and Lu, Mingyu and Duan, Qing and Liu, Junhui},
  date = {2025-08-02},
  journaltitle = {ACM Computing Surveys},
  shortjournal = {ACM Comput. Surv.},
  pages = {3758085},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3758085},
  url = {https://dl.acm.org/doi/10.1145/3758085},
  urldate = {2025-08-25},
  abstract = {View synthesis is a fundamental task in computer vision, known for its significantly higher complexity compared to conventional vision problems. The introduction of Neural Radiance Fields (NeRF) marked a major breakthrough in this field, substantially improving previous methods and pushing view synthesis to unprecedented levels. This survey aims to systematically review the progress of NeRF-based models in computer vision. We begin by explaining the core principles underlying the success of NeRF. Then, we delve into and analyze seven representative NeRF-based representation forms, including Implicit Representation, Neural Point Cloud, and others. Next, we provide a comprehensive comparison and analysis of 14 major research directions that enhance NeRF, such as Modeling Different Practical Capturing Scenarios, Generalization in Modeling, and Modeling Dynamic Scenes. In addition, we conduct both qualitative and quantitative evaluations of numerous NeRF-based methods on multiple datasets, comparing training time, rendering speed, and memory requirements. Finally, we discuss potential future research directions and challenges in this field. We hope that this work will inspire further interest and contribute to advancing the application and development of NeRF in computer vision.},
  langid = {english}
}

@article{lin2022,
  title = {Generalized Resampled Importance Sampling: Foundations of {{ReSTIR}}},
  shorttitle = {Generalized Resampled Importance Sampling},
  author = {Lin, Daqi and Kettunen, Markus and Bitterli, Benedikt and Pantaleoni, Jacopo and Yuksel, Cem and Wyman, Chris},
  date = {2022-07},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {41},
  number = {4},
  pages = {1--23},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/3528223.3530158},
  url = {https://dl.acm.org/doi/10.1145/3528223.3530158},
  urldate = {2025-04-30},
  abstract = {As scenes become ever more complex and real-time applications embrace ray tracing, path sampling algorithms that maximize quality at low sample counts become vital. Recent               resampling               algorithms building on Talbot et al.'s [2005] resampled importance sampling (RIS) reuse paths spatiotemporally to render surprisingly complex light transport with a few samples per pixel. These reservoir-based spatiotemporal importance resamplers (ReSTIR) and their underlying RIS theory make various assumptions, including sample independence. But sample reuse               introduces correlation               , so ReSTIR-style iterative reuse loses most convergence guarantees that RIS theoretically provides.                          We introduce generalized resampled importance sampling (GRIS) to extend the theory, allowing RIS on correlated samples, with unknown PDFs and taken from varied domains. This solidifies the theoretical foundation, allowing us to derive variance bounds and convergence conditions in ReSTIR-based samplers. It also guides practical algorithm design and enables advanced path reuse between pixels via complex shift mappings.             We show a path-traced resampler (ReSTIR PT) running interactively on complex scenes, capturing many-bounce diffuse and specular lighting while shading just one path per pixel. With our new theoretical foundation, we can also modify the algorithm to guarantee convergence for offline renderers.},
  langid = {english},
  file = {/Users/julianstamm/Zotero/storage/3GP57JFU/Lin et al. - 2022 - Generalized resampled importance sampling foundations of ReSTIR.pdf}
}

@article{lin2023,
  title = {Hypothesis {{Testing}} for {{Progressive Kernel Estimation}} and {{VCM Framework}}},
  author = {Lin, Zehui and Hu, Chenxiao and Jia, Jinzhu and Li, Sheng},
  date = {2023},
  journaltitle = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {30},
  number = {8},
  pages = {4709--4723},
  publisher = {IEEE},
  url = {https://ieeexplore.ieee.org/abstract/document/10122174/},
  urldate = {2025-08-21},
  file = {/Users/julianstamm/Zotero/storage/2EBY5CVP/Lin et al. - 2023 - Hypothesis Testing for Progressive Kernel Estimation and VCM Framework.pdf}
}

@incollection{lyu2022,
  title = {Neural {{Radiance Transfer Fields}} for {{Relightable Novel-View Synthesis}} with {{Global Illumination}}},
  booktitle = {Computer {{Vision}} – {{ECCV}} 2022},
  author = {Lyu, Linjie and Tewari, Ayush and Leimkühler, Thomas and Habermann, Marc and Theobalt, Christian},
  editor = {Avidan, Shai and Brostow, Gabriel and Cissé, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
  date = {2022},
  volume = {13677},
  pages = {153--169},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-19790-1_10},
  url = {https://link.springer.com/10.1007/978-3-031-19790-1_10},
  urldate = {2025-08-25},
  isbn = {978-3-031-19789-5 978-3-031-19790-1},
  langid = {english},
  file = {/Users/julianstamm/Zotero/storage/78MTTZM9/Lyu et al. - 2022 - Neural Radiance Transfer Fields for Relightable Novel-View Synthesis with Global Illumination.pdf}
}

@inproceedings{magdon-ismail1998,
  title = {Neural {{Networks}} for {{Density Estimation}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Magdon-Ismail, Malik and Atiya, Amir},
  date = {1998},
  volume = {11},
  publisher = {MIT Press},
  url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/9327969053c0068dd9e07c529866b94d-Abstract.html},
  urldate = {2025-05-04},
  abstract = {We  introduce two  new  techniques for  density estimation.  Our ap(cid:173) proach poses the problem as  a  supervised learning task which  can  be  performed  using  Neural  Networks.  We  introduce  a  stochas(cid:173) tic method for  learning the cumulative distribution  and an  analo(cid:173) gous  deterministic technique.  We  demonstrate convergence of our  methods  both theoretically and experimentally, and provide com(cid:173) parisons with the Parzen estimate.  Our theoretical results demon(cid:173) strate better convergence properties than the Parzen estimate.},
  file = {/Users/julianstamm/Zotero/storage/9636C9J9/Magdon-Ismail and Atiya - 1998 - Neural Networks for Density Estimation.pdf}
}

@article{majercik2019,
  title = {Dynamic Diffuse Global Illumination with Ray-Traced Irradiance Fields},
  author = {Majercik, Zander and Guertin, Jean-Philippe and Nowrouzezahrai, Derek and McGuire, Morgan},
  date = {2019},
  journaltitle = {Journal of Computer Graphics Techniques},
  volume = {8},
  number = {2},
  url = {https://cim.mcgill.ca/~derek/files/DDGI-lowres.pdf},
  urldate = {2025-07-29},
  file = {/Users/julianstamm/Zotero/storage/SGGZFATX/Majercik et al. - 2019 - Dynamic diffuse global illumination with ray-traced irradiance fields.pdf}
}

@inproceedings{majercik2021,
  title = {Dynamic {{Diffuse Global Illumination Resampling}}},
  booktitle = {{{ACM SIGGRAPH}} 2021 {{Talks}}},
  author = {Majercik, Zander and Müller, Thomas and Keller, Alexander and Nowrouzezahrai, Derek and McGuire, Morgan},
  date = {2021-07-31},
  pages = {1--2},
  publisher = {ACM},
  location = {Virtual Event USA},
  doi = {10.1145/3450623.3464635},
  url = {https://dl.acm.org/doi/10.1145/3450623.3464635},
  urldate = {2025-07-29},
  eventtitle = {{{SIGGRAPH}} '21: {{Special Interest Group}} on {{Computer Graphics}} and {{Interactive Techniques Conference}}},
  isbn = {978-1-4503-8373-8},
  langid = {english},
  file = {/Users/julianstamm/Zotero/storage/AEMPQYJF/Majercik et al. - 2021 - Dynamic Diffuse Global Illumination Resampling.pdf;/Users/julianstamm/Zotero/storage/67GIU2IK/3450623.html}
}

@inproceedings{mara2013,
  title = {Toward Practical Real-Time Photon Mapping: Efficient {{GPU}} Density Estimation},
  shorttitle = {Toward Practical Real-Time Photon Mapping},
  booktitle = {Proceedings of the {{ACM SIGGRAPH Symposium}} on {{Interactive 3D Graphics}} and {{Games}}},
  author = {Mara, Michael and Luebke, David and McGuire, Morgan},
  date = {2013-03-21},
  series = {{{I3D}} '13},
  pages = {71--78},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2448196.2448207},
  url = {https://dl.acm.org/doi/10.1145/2448196.2448207},
  urldate = {2025-05-17},
  abstract = {We describe the design space for real-time photon density estimation, the key step of rendering global illumination (GI) via photon mapping. We then detail and analyze efficient GPU implementations of four best-of-breed algorithms. All produce reasonable results on NVIDIA GeForce 670 at 1920 × 1080 for complex scenes with multiple-bounce diffuse effects, caustics, and glossy reflection in real-time. Across the designs we conclude that tiled, deferred photon gathering in a compute shader gives the best combination of performance and quality.},
  isbn = {978-1-4503-1956-0},
  file = {/Users/julianstamm/Zotero/storage/SYH49PPN/Mara et al. - 2013 - Toward practical real-time photon mapping efficient GPU density estimation.pdf}
}

@article{martin2015,
  title = {{{TensorFlow}}: {{Large-scale}} Machine Learning on Heterogeneous Systems},
  shorttitle = {{{TensorFlow}}},
  author = {Martín, Abadi and Ashish, Agarwal and Paul, Barham and Eugene, Brevdo and Zhifeng, Chen and Craig, Citro and Greg, S. Corrado and Andy, Davis and Jeffrey, Dean and Matthieu, Devin},
  date = {2015},
  journaltitle = {Software available from tensorflow. org},
  volume = {7},
  url = {tensorflow.org}
}

@inproceedings{mcguire2017,
  title = {Real-Time Global Illumination Using Precomputed Light Field Probes},
  booktitle = {Proceedings of the 21st {{ACM SIGGRAPH Symposium}} on {{Interactive 3D Graphics}} and {{Games}}},
  author = {McGuire, Morgan and Mara, Mike and Nowrouzezahrai, Derek and Luebke, David},
  date = {2017-02-25},
  pages = {1--11},
  publisher = {ACM},
  location = {San Francisco California},
  doi = {10.1145/3023368.3023378},
  url = {https://dl.acm.org/doi/10.1145/3023368.3023378},
  urldate = {2025-08-22},
  eventtitle = {{{I3D}} '17: {{Symposium}} on {{Interactive 3D Graphics}} and {{Games}}},
  isbn = {978-1-4503-4886-7},
  langid = {english},
  file = {/Users/julianstamm/Zotero/storage/YL26TAKJ/McGuire et al. - 2017 - Real-time global illumination using precomputed light field probes.pdf}
}

@article{metropolis1949,
  title = {The {{Monte Carlo Method}}},
  author = {Metropolis, Nicholas and Ulam, S.},
  date = {1949-09},
  journaltitle = {Journal of the American Statistical Association},
  shortjournal = {Journal of the American Statistical Association},
  volume = {44},
  number = {247},
  pages = {335--341},
  issn = {0162-1459, 1537-274X},
  doi = {10.1080/01621459.1949.10483310},
  url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1949.10483310},
  urldate = {2025-08-08},
  langid = {english},
  file = {/Users/julianstamm/Zotero/storage/47TTW5A9/Metropolis and Ulam - 1949 - The Monte Carlo Method.pdf}
}

@online{mihut,
  type = {Repository},
  title = {{{NRC Integration Guide}}},
  author = {Mihuț, Ana},
  url = {https://github.com/NVIDIA-RTX/RTXGI/blob/main/Docs/NrcGuide.md},
  urldate = {2025-03-22},
  langid = {english},
  organization = {Github},
  file = {/Users/julianstamm/Zotero/storage/74F5A66G/NrcGuide.html}
}

@online{mikkelsen,
  title = {{{MikkTSpace}}.Com},
  author = {Mikkelsen, Morten S.},
  url = {http://www.mikktspace.com/},
  urldate = {2024-08-28},
  file = {/Users/julianstamm/Zotero/storage/M7WN8MUS/www.mikktspace.com.html}
}

@online{mildenhall2020,
  title = {{{NeRF}}: {{Representing Scenes}} as {{Neural Radiance Fields}} for {{View Synthesis}}},
  shorttitle = {{{NeRF}}},
  author = {Mildenhall, Ben and Srinivasan, Pratul P. and Tancik, Matthew and Barron, Jonathan T. and Ramamoorthi, Ravi and Ng, Ren},
  date = {2020-08-03},
  eprint = {2003.08934},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2003.08934},
  url = {http://arxiv.org/abs/2003.08934},
  urldate = {2025-08-25},
  abstract = {We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location \$(x,y,z)\$ and viewing direction \$(\textbackslash theta, \textbackslash phi)\$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  file = {/Users/julianstamm/Zotero/storage/BUZPTTMH/Mildenhall et al. - 2020 - NeRF Representing Scenes as Neural Radiance Fields for View Synthesis.pdf;/Users/julianstamm/Zotero/storage/NX2XP3W8/Mildenhall et al. - 2022 - NeRF representing scenes as neural radiance fields for view synthesis.pdf;/Users/julianstamm/Zotero/storage/WJIJ5KDK/2003.html}
}

@inproceedings{moreau2019,
  title = {Dynamic {{Many-Light Sampling}} for {{Real-Time Ray Tracing}}},
  booktitle = {High {{Performance Graphics}} ({{Short Papers}})},
  author = {Moreau, Pierre and Pharr, Matt and Clarberg, Petrik},
  date = {2019},
  pages = {21--26},
  url = {https://research.nvidia.com/sites/default/files/pubs/2019-07_Dynamic-Many-Light-Sampling//MPC19.pdf},
  langid = {english},
  file = {/Users/julianstamm/Zotero/storage/HGVN2WCJ/Moreau et al. - 2019 - Dynamic Many-Light Sampling for Real-Time Ray Tracing..pdf;/Users/julianstamm/Zotero/storage/XUC89UX2/Moreau et al. - 2019 - Dynamic Many-Light Sampling for Real-Time Ray Tracing.pdf}
}

@article{muller2017,
  title = {Practical {{Path Guiding}} for {{Efficient Light}}‐{{Transport Simulation}}},
  author = {Müller, Thomas and Gross, Markus and Novák, Jan},
  date = {2017-07},
  journaltitle = {Computer Graphics Forum},
  shortjournal = {Computer Graphics Forum},
  volume = {36},
  number = {4},
  pages = {91--100},
  issn = {0167-7055, 1467-8659},
  doi = {10.1111/cgf.13227},
  url = {https://onlinelibrary.wiley.com/doi/10.1111/cgf.13227},
  urldate = {2025-05-16},
  abstract = {We present a robust, unbiased technique for intelligent light-path construction in path-tracing algorithms. Inspired by existing path-guiding algorithms, our method learns an approximate representation of the scene’s spatio-directional radiance field in an unbiased and iterative manner. To that end, we propose an adaptive spatio-directional hybrid data structure, referred to as SD-tree, for storing and sampling incident radiance. The SD-tree consists of an upper part—a binary tree that partitions the 3D spatial domain of the light field—and a lower part—a quadtree that partitions the 2D directional domain. We further present a principled way to automatically budget training and rendering computations to minimize the variance of the final image. Our method does not require tuning hyperparameters, although we allow limiting the memory footprint of the SD-tree. The aforementioned properties, its ease of implementation, and its stable performance make our method compatible with production environments. We demonstrate the merits of our method on scenes with difficult visibility, detailed geometry, and complex specular-glossy light transport, achieving better performance than previous state-of-the-art algorithms.},
  langid = {english},
  file = {/Users/julianstamm/Zotero/storage/6MUGG4B9/Müller et al. - 2017 - Practical Path Guiding for Efficient Light‐Transport Simulation.pdf}
}

@online{muller2019,
  title = {Neural {{Importance Sampling}}},
  author = {Müller, Thomas and McWilliams, Brian and Rousselle, Fabrice and Gross, Markus and Novák, Jan},
  date = {2019-09-03},
  eprint = {1808.03856},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1808.03856},
  url = {http://arxiv.org/abs/1808.03856},
  urldate = {2025-04-23},
  abstract = {We propose to use deep neural networks for generating samples in Monte Carlo integration. Our work is based on non-linear independent components estimation (NICE), which we extend in numerous ways to improve performance and enable its application to integration problems. First, we introduce piecewise-polynomial coupling transforms that greatly increase the modeling power of individual coupling layers. Second, we propose to preprocess the inputs of neural networks using one-blob encoding, which stimulates localization of computation and improves inference. Third, we derive a gradient-descent-based optimization for the KL and the \$\textbackslash chi\textasciicircum 2\$ divergence for the specific application of Monte Carlo integration with unnormalized stochastic estimates of the target distribution. Our approach enables fast and accurate inference and efficient sample generation independently of the dimensionality of the integration domain. We show its benefits on generating natural images and in two applications to light-transport simulation: first, we demonstrate learning of joint path-sampling densities in the primary sample space and importance sampling of multi-dimensional path prefixes thereof. Second, we use our technique to extract conditional directional densities driven by the product of incident illumination and the BSDF in the rendering equation, and we leverage the densities for path guiding. In all applications, our approach yields on-par or higher performance than competing techniques at equal sample count.},
  pubstate = {prepublished},
  keywords = {Computer Science - Graphics,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/julianstamm/Zotero/storage/E65E8WYE/Müller et al. - 2019 - Neural Importance Sampling.pdf;/Users/julianstamm/Zotero/storage/EPNMQZR3/1808.html}
}

@online{muller2020,
  title = {Neural {{Control Variates}}},
  author = {Müller, Thomas and Rousselle, Fabrice and Novák, Jan and Keller, Alexander},
  date = {2020-09-04},
  eprint = {2006.01524},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2006.01524},
  url = {http://arxiv.org/abs/2006.01524},
  urldate = {2025-07-06},
  abstract = {We propose neural control variates (NCV) for unbiased variance reduction in parametric Monte Carlo integration. So far, the core challenge of applying the method of control variates has been finding a good approximation of the integrand that is cheap to integrate. We show that a set of neural networks can face that challenge: a normalizing flow that approximates the shape of the integrand and another neural network that infers the solution of the integral equation. We also propose to leverage a neural importance sampler to estimate the difference between the original integrand and the learned control variate. To optimize the resulting parametric estimator, we derive a theoretically optimal, variance-minimizing loss function, and propose an alternative, composite loss for stable online training in practice. When applied to light transport simulation, neural control variates are capable of matching the state-of-the-art performance of other unbiased approaches, while providing means to develop more performant, practical solutions. Specifically, we show that the learned light-field approximation is of sufficient quality for high-order bounces, allowing us to omit the error correction and thereby dramatically reduce the noise at the cost of negligible visible bias.},
  pubstate = {prepublished},
  keywords = {Computer Science - Graphics,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/julianstamm/Zotero/storage/J9YS54S2/Müller et al. - 2020 - Neural Control Variates.pdf;/Users/julianstamm/Zotero/storage/RBZT2ID6/2006.html}
}

@article{muller2021,
  title = {Real-Time Neural Radiance Caching for Path Tracing},
  author = {Müller, Thomas and Rousselle, Fabrice and Novák, Jan and Keller, Alexander},
  date = {2021-08-31},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {40},
  number = {4},
  pages = {1--16},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/3450626.3459812},
  url = {https://dl.acm.org/doi/10.1145/3450626.3459812},
  urldate = {2024-09-24},
  abstract = {We present a real-time neural radiance caching method for path-traced global illumination. Our system is designed to handle fully dynamic scenes, and makes no assumptions about the lighting, geometry, and materials. The data-driven nature of our approach sidesteps many difficulties of caching algorithms, such as locating, interpolating, and updating cache points. Since pretraining neural networks to handle novel, dynamic scenes is a formidable generalization challenge, we do away with pretraining and instead achieve               generalization via adaptation               , i.e. we opt for training the radiance cache while rendering. We employ self-training to provide low-noise training targets and simulate infinite-bounce transport by merely iterating few-bounce training updates. The updates and cache queries incur a mild overhead---about 2.6ms on full HD resolution---thanks to a streaming implementation of the neural network that fully exploits modern hardware. We demonstrate significant noise reduction at the cost of little induced bias, and report state-of-the-art, real-time performance on a number of challenging scenarios.},
  langid = {english},
  keywords = {Main},
  file = {/Users/julianstamm/Zotero/storage/RCM58VU2/Müller et al. - 2021 - Real-time neural radiance caching for path tracing.pdf}
}

@article{muller2022,
  title = {Instant Neural Graphics Primitives with a Multiresolution Hash Encoding},
  author = {Müller, Thomas and Evans, Alex and Schied, Christoph and Keller, Alexander},
  date = {2022-07},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {41},
  number = {4},
  pages = {1--15},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/3528223.3530127},
  url = {https://dl.acm.org/doi/10.1145/3528223.3530127},
  urldate = {2025-04-22},
  abstract = {Neural graphics primitives, parameterized by fully connected neural networks, can be costly to train and evaluate. We reduce this cost with a versatile new input encoding that permits the use of a smaller network without sacrificing quality, thus significantly reducing the number of floating point and memory access operations: a small neural network is augmented by a multiresolution hash table of trainable feature vectors whose values are optimized through stochastic gradient descent. The multiresolution structure allows the network to disambiguate hash collisions, making for a simple architecture that is trivial to parallelize on modern GPUs. We leverage this parallelism by implementing the whole system using fully-fused CUDA kernels with a focus on minimizing wasted bandwidth and compute operations. We achieve a combined speedup of several orders of magnitude, enabling training of high-quality neural graphics primitives in a matter of seconds, and rendering in tens of milliseconds at a resolution of 1920×1080.},
  langid = {english},
  file = {/Users/julianstamm/Zotero/storage/G6CDDTGE/Müller et al. - 2022 - Instant neural graphics primitives with a multiresolution hash encoding.pdf}
}

@article{nalbach2017,
  title = {Deep {{Shading}}: {{Convolutional Neural Networks}} for {{Screen Space Shading}}},
  shorttitle = {Deep {{Shading}}},
  author = {Nalbach, O. and Arabadzhiyska, E. and Mehta, D. and Seidel, H.‐P. and Ritschel, T.},
  date = {2017-07},
  journaltitle = {Computer Graphics Forum},
  shortjournal = {Computer Graphics Forum},
  volume = {36},
  number = {4},
  pages = {65--78},
  issn = {0167-7055, 1467-8659},
  doi = {10.1111/cgf.13225},
  url = {https://onlinelibrary.wiley.com/doi/10.1111/cgf.13225},
  urldate = {2025-08-25},
  abstract = {Abstract             In computer vision, convolutional neural networks (CNNs) achieve unprecedented performance for inverse problems where RGB pixel appearance is mapped to attributes such as positions, normals or reflectance. In computer graphics, screen space shading has boosted the quality of real‐time rendering, converting the same kind of attributes of a virtual scene back to appearance, enabling effects like ambient occlusion, indirect light, scattering and many more. In this paper we consider the diagonal problem: synthesizing appearance from given per‐pixel attributes using a CNN. The resulting Deep Shading renders screen space effects at competitive quality and speed while not being programmed by human experts but learned from example images.},
  langid = {english},
  file = {/Users/julianstamm/Zotero/storage/4858BQ9Q/Nalbach et al. - 2017 - Deep Shading Convolutional Neural Networks for Screen Space Shading.pdf}
}

@report{nvidiacorporation2024,
  title = {{{cuRAND Library Programming Guide}}},
  author = {{NVIDIA Corporation}},
  date = {2024-01},
  url = {https://docs.nvidia.com/cuda/pdf/CURAND_Library.pdf},
  file = {/Users/julianstamm/Zotero/storage/5SVTVRWP/CURAND_Library.pdf}
}

@software{nvidiagameworksrtxgiddgi2025,
  title = {{{NVIDIAGameWorks}}/{{RTXGI-DDGI}}},
  date = {2025-08-22T01:31:18Z},
  origdate = {2020-02-24T21:02:19Z},
  url = {https://github.com/NVIDIAGameWorks/RTXGI-DDGI},
  urldate = {2025-08-24},
  abstract = {RTX Global Illumination (RTXGI)},
  organization = {NVIDIA GameWorks},
  keywords = {global-illumination,ray-tracing,rtx,rtxgi}
}

@unpublished{oat2005,
  title = {Irradiance {{Volumes For Games}}},
  author = {Oat},
  date = {2005},
  file = {/Users/julianstamm/Zotero/storage/LSMXV8PD/Oat_GDC2005_IrradianceVolumesForGames.pdf}
}

@inproceedings{odonnell2018,
  title = {Precomputed {{Global Illumination}} in {{Frostbite}}},
  booktitle = {Game {{Developers Conference}}},
  author = {O’Donnell, Yuriy},
  date = {2018},
  volume = {1},
  number = {2},
  langid = {english},
  file = {/Users/julianstamm/Zotero/storage/MNPS8WJW/O'Donnell - Precomputed Global Illumination in Frostbite.pdf}
}

@inproceedings{oren1994,
  title = {Generalization of {{Lambert}}'s Reflectance Model},
  booktitle = {Proceedings of the 21st Annual Conference on {{Computer}} Graphics and Interactive Techniques  - {{SIGGRAPH}} '94},
  author = {Oren, Michael and Nayar, Shree K.},
  date = {1994},
  pages = {239--246},
  publisher = {ACM Press},
  location = {Not Known},
  doi = {10.1145/192161.192213},
  url = {http://portal.acm.org/citation.cfm?doid=192161.192213},
  urldate = {2025-06-24},
  eventtitle = {The 21st Annual Conference},
  isbn = {978-0-89791-667-7},
  langid = {english},
  file = {/Users/julianstamm/Zotero/storage/RJ23AJIH/Oren and Nayar - 1994 - Generalization of Lambert's reflectance model.pdf}
}

@article{ouyang2021,
  title = {{{ReSTIR GI}}: {{Path Resampling}} for {{Real}}‐{{Time Path Tracing}}},
  shorttitle = {{{ReSTIR GI}}},
  author = {Ouyang, Y. and Liu, S. and Kettunen, M. and Pharr, M. and Pantaleoni, J.},
  date = {2021-12},
  journaltitle = {Computer Graphics Forum},
  shortjournal = {Computer Graphics Forum},
  volume = {40},
  number = {8},
  pages = {17--29},
  issn = {0167-7055, 1467-8659},
  doi = {10.1111/cgf.14378},
  url = {https://onlinelibrary.wiley.com/doi/10.1111/cgf.14378},
  urldate = {2025-04-22},
  abstract = {Even with the advent of hardware-accelerated ray tracing in modern GPUs, only a small number of rays can be traced at each pixel in real-time applications. This presents a significant challenge for path tracing, even when augmented with state-of-the art denoising algorithms. While the recently-developed ReSTIR algorithm [BWP∗20] enables high-quality renderings of scenes with millions of light sources using just a few shadow rays at each pixel, there remains a need for effective algorithms to sample indirect illumination.},
  langid = {english},
  file = {/Users/julianstamm/Zotero/storage/SATQFIQP/Ouyang et al. - 2021 - ReSTIR GI Path Resampling for Real‐Time Path Tracing.pdf}
}

@inproceedings{owen1995,
  title = {Randomly {{Permuted}} (t,m,s)-{{Nets}} and (t, s)-{{Sequences}}},
  booktitle = {Monte {{Carlo}} and {{Quasi-Monte Carlo Methods}} in {{Scientific Computing}}},
  author = {Owen, Art B.},
  editor = {Niederreiter, Harald and Shiue, Peter Jau-Shyong},
  date = {1995},
  pages = {299--317},
  publisher = {Springer},
  location = {New York, NY},
  doi = {10.1007/978-1-4612-2552-2_19},
  abstract = {This article presents a hybrid of Monte Carlo and Quasi-Monte Carlo methods. In this hybrid, certain low discrepancy point sets and sequences due to Faure, Niederreiter and Sobol’ are obtained and their digits are randomly permuted. Since this randomization preserves the equidistribution properties of the points it also preserves the proven bounds on their quadrature errors. The accuracy of an estimated integrand can be assessed by replication, consisting of independent re-randomizations.},
  isbn = {978-1-4612-2552-2},
  langid = {english}
}

@article{owen2008,
  title = {Local Antithetic Sampling with Scrambled Nets},
  author = {Owen, Art B.},
  date = {2008-10-01},
  journaltitle = {The Annals of Statistics},
  shortjournal = {Ann. Statist.},
  volume = {36},
  number = {5},
  eprint = {0811.0528},
  eprinttype = {arXiv},
  eprintclass = {stat},
  issn = {0090-5364},
  doi = {10.1214/07-AOS548},
  url = {http://arxiv.org/abs/0811.0528},
  urldate = {2025-06-15},
  abstract = {We consider the problem of computing an approximation to the integral \$I=\textbackslash int\_\{[0,1]\textasciicircum d\}f(x) dx\$. Monte Carlo (MC) sampling typically attains a root mean squared error (RMSE) of \$O(n\textasciicircum\{-1/2\})\$ from \$n\$ independent random function evaluations. By contrast, quasi-Monte Carlo (QMC) sampling using carefully equispaced evaluation points can attain the rate \$O(n\textasciicircum\{-1+\textbackslash varepsilon\})\$ for any \$\textbackslash varepsilon{$>$}0\$ and randomized QMC (RQMC) can attain the RMSE \$O(n\textasciicircum\{-3/2+\textbackslash varepsilon\})\$, both under mild conditions on \$f\$. Classical variance reduction methods for MC can be adapted to QMC. Published results combining QMC with importance sampling and with control variates have found worthwhile improvements, but no change in the error rate. This paper extends the classical variance reduction method of antithetic sampling and combines it with RQMC. One such method is shown to bring a modest improvement in the RMSE rate, attaining \$O(n\textasciicircum\{-3/2-1/d+\textbackslash varepsilon\})\$ for any \$\textbackslash varepsilon{$>$}0\$, for smooth enough \$f\$.},
  keywords = {Mathematics - Statistics Theory,Statistics - Computation,Statistics - Statistics Theory},
  file = {/Users/julianstamm/Zotero/storage/RKHBFXPF/Owen - 2008 - Local antithetic sampling with scrambled nets.pdf;/Users/julianstamm/Zotero/storage/CQ4H5JJ3/0811.html}
}

@article{pantaleoni2020,
  title = {Online {{Path Sampling Control}} with {{Progressive Spatio-temporal Filtering}}},
  author = {Pantaleoni, Jacopo},
  date = {2020-09},
  journaltitle = {SN Computer Science},
  shortjournal = {SN COMPUT. SCI.},
  volume = {1},
  number = {5},
  pages = {279},
  issn = {2662-995X, 2661-8907},
  doi = {10.1007/s42979-020-00291-z},
  url = {https://link.springer.com/10.1007/s42979-020-00291-z},
  urldate = {2025-06-27},
  langid = {english},
  file = {/Users/julianstamm/Zotero/storage/EZKEZXPF/Pantaleoni - 2020 - Online Path Sampling Control with Progressive Spatio-temporal Filtering.pdf}
}

@book{pharr2023,
  title = {Physically Based Rendering: {{From}} Theory to Implementation},
  shorttitle = {Physically Based Rendering},
  author = {Pharr, Matt and Jakob, Wenzel and Humphreys, Greg},
  date = {2023},
  publisher = {MIT Press},
  url = {https://books.google.com/books?hl=en&lr=&id=i9d2EAAAQBAJ&oi=fnd&pg=PR17&dq=physically+based+rendering+from+theory&ots=eF2MZlw6xm&sig=pmi3D6QVEXMfSRJJiySq-Fd99Fw},
  urldate = {2025-06-26}
}

@online{portsmouth2024,
  title = {{{EON}}: {{A}} Practical Energy-Preserving Rough Diffuse {{BRDF}}},
  shorttitle = {{{EON}}},
  author = {Portsmouth, Jamie and Kutz, Peter and Hill, Stephen},
  date = {2024-11-03},
  eprint = {2410.18026},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2410.18026},
  url = {http://arxiv.org/abs/2410.18026},
  urldate = {2025-07-03},
  abstract = {We introduce the "Energy-preserving Oren--Nayar" (EON) model for reflection from rough surfaces. Unlike the popular qualitative Oren--Nayar model (QON) and its variants, our model is energy-preserving via analytical energy compensation. We include self-contained GLSL source code for efficient evaluation of the new model and importance sampling based on a novel technique we term "Clipped Linearly Transformed Cosine" (CLTC) sampling.},
  pubstate = {prepublished},
  keywords = {Computer Science - Graphics},
  file = {/Users/julianstamm/Zotero/storage/S5HDZAL7/Portsmouth et al. - 2024 - EON A practical energy-preserving rough diffuse BRDF.pdf;/Users/julianstamm/Zotero/storage/A7G2YKMG/2410.html}
}

@article{qin2015,
  title = {Unbiased Photon Gathering for Light Transport Simulation},
  author = {Qin, Hao and Sun, Xin and Hou, Qiming and Guo, Baining and Zhou, Kun},
  date = {2015-11-04},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {34},
  number = {6},
  pages = {1--14},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/2816795.2818119},
  url = {https://dl.acm.org/doi/10.1145/2816795.2818119},
  urldate = {2025-05-13},
  abstract = {Photon mapping (PM) has been widely regarded as an efficient solution for light transport simulation, including challenging caustics paths and many-bounce indirect lighting. The efficiency of PM comes from reusing traced photons. However, the handling of photon gathering in existing PM algorithms is universally biased -- the expected value of their results does not necessarily agree with the true solution of the rendering equation. We present a novel photon gathering method to efficiently achieve unbiased rendering with photon mapping. Instead of aggregating the gathered photons into an estimated density as in classical photon mapping, we process each photon individually and connect the corresponding light sub-path with the eye sub-path that generates the gather point, creating an unbiased path sample. The Monte Carlo estimate for such a path sample is calculated by evaluating all relevant terms in a strict and unbiased way, leading to a self-contained unbiased sampling technique. We further develop a set of multiple importance sampling (MIS) weights that allow our method to be optimally combined with bidirectional path tracing (BDPT), resulting in an unbiased rendering algorithm that can efficiently handle a wide variety of light paths and that compares favorably with previous algorithms. Experiments demonstrate the efficacy and robustness of our method.},
  langid = {english},
  file = {/Users/julianstamm/Zotero/storage/K32QSZVB/Qin et al. - 2015 - Unbiased photon gathering for light transport simulation.pdf}
}

@article{ren2013,
  title = {Global Illumination with Radiance Regression Functions},
  author = {Ren, Peiran and Wang, Jiaping and Gong, Minmin and Lin, Stephen and Tong, Xin and Guo, Baining},
  date = {2013},
  journaltitle = {ACM Trans. Graph.},
  volume = {32},
  number = {4},
  pages = {130--1},
  url = {https://www.academia.edu/download/54395664/Global_illumination_with_radiance_regres20170910-8835-10klmux.pdf},
  urldate = {2025-08-25},
  file = {/Users/julianstamm/Zotero/storage/KLKSSXFL/Ren et al. - 2013 - Global illumination with radiance regression functions.pdf;/Users/julianstamm/Zotero/storage/QZKISVSE/2461912.html}
}

@online{reynolds2016,
  title = {Orthonormal Basis from Normal via Quaternion Similarity},
  author = {Reynolds, Marc B.},
  date = {2016-07-06T00:00:00+00:00},
  url = {http://marc-b-reynolds.github.io/quaternions/2016/07/06/Orthonormal.html},
  urldate = {2024-08-28},
  abstract = {two methods of computing an orthonormal basis from a unit (bi)vector.},
  organization = {Marc-B-Reynolds.github.io},
  file = {/Users/julianstamm/Zotero/storage/LJGWWJDN/Orthonormal.html}
}

@article{ritschel2012,
  title = {The {{State}} of the {{Art}} in {{Interactive Global Illumination}}},
  author = {Ritschel, Tobias and Dachsbacher, Carsten and Grosch, Thorsten and Kautz, Jan},
  date = {2012-02},
  journaltitle = {Computer Graphics Forum},
  shortjournal = {Computer Graphics Forum},
  volume = {31},
  number = {1},
  pages = {160--188},
  issn = {0167-7055, 1467-8659},
  doi = {10.1111/j.1467-8659.2012.02093.x},
  url = {https://onlinelibrary.wiley.com/doi/10.1111/j.1467-8659.2012.02093.x},
  urldate = {2025-05-04},
  abstract = {The interaction of light and matter in the world surrounding us is of striking complexity and beauty. Since the very beginning of computer graphics, adequate modeling of these processes and efficient computation is an intensively studied research topic and still not a solved problem. The inherent complexity stems from the underlying physical processes as well as the global nature of the interactions that let light travel within a scene. This article reviews the state of the art in interactive global illumination computation, that is, methods that generate an image of a virtual scene in less than one second with an as exact as possible, or plausible, solution to the light transport. Additionally, the theoretical background and attempts to classify the broad field of methods are described. The strengths and weaknesses of different approaches, when applied to the different visual phenomena, arising from light interaction are compared and discussed. Finally, the article concludes by highlighting design patterns for interactive global illumination and a list of open problems.},
  langid = {english},
  file = {/Users/julianstamm/Zotero/storage/AB8BRB2D/Ritschel et al. - 2012 - The State of the Art in Interactive Global Illumination.pdf}
}

@inproceedings{schied2017,
  title = {Spatiotemporal Variance-Guided Filtering: Real-Time Reconstruction for Path-Traced Global Illumination},
  shorttitle = {Spatiotemporal Variance-Guided Filtering},
  booktitle = {Proceedings of {{High Performance Graphics}}},
  author = {Schied, Christoph and Kaplanyan, Anton and Wyman, Chris and Patney, Anjul and Chaitanya, Chakravarty R. Alla and Burgess, John and Liu, Shiqiu and Dachsbacher, Carsten and Lefohn, Aaron and Salvi, Marco},
  date = {2017-07-28},
  pages = {1--12},
  publisher = {ACM},
  location = {Los Angeles California},
  doi = {10.1145/3105762.3105770},
  url = {https://dl.acm.org/doi/10.1145/3105762.3105770},
  urldate = {2025-08-25},
  eventtitle = {{{HPG}} '17: {{High-Performance Graphics}}},
  isbn = {978-1-4503-5101-0},
  langid = {english},
  file = {/Users/julianstamm/Zotero/storage/ADFMDATX/Schied et al. - 2017 - Spatiotemporal variance-guided filtering real-time reconstruction for path-traced global illuminati.pdf}
}

@article{schlick1994,
  title = {An {{Inexpensive BRDF Model}} for {{Physically}}‐based {{Rendering}}},
  author = {Schlick, Christophe},
  date = {1994-08},
  journaltitle = {Computer Graphics Forum},
  shortjournal = {Computer Graphics Forum},
  volume = {13},
  number = {3},
  pages = {233--246},
  issn = {0167-7055, 1467-8659},
  doi = {10.1111/1467-8659.1330233},
  url = {https://onlinelibrary.wiley.com/doi/10.1111/1467-8659.1330233},
  urldate = {2025-06-24},
  abstract = {Abstract:                            A new BRDF model is presented which can be viewed as an kind of intermediary model between empirism and theory. Main results of physics are observed (energy conservation, reciprocity rule, microfacet theory) and numerous phenomena involved in light reflection are accounted for, in a physically plausible way (incoherent and coherent reflection, spectrum modifications, anisotropy, self‐shadowing, multiple surface and subsurface reflection, differences between homogeneous and heterogeneous materials). The model has been especially intended for computer graphics applications and therefore includes two main features:               simplicity               (a small number of intuitively understandable parameters controls the model) and efficiency (the formulation provides adequation to Monte‐Carlo rendering techniques and/or hardware implementations).},
  langid = {english},
  keywords = {Bidirectional Reflectance Distribution Function,Optimization,Physically-Based Rendering},
  file = {/Users/julianstamm/Zotero/storage/YKPL24U6/Schlick - 1994 - An Inexpensive BRDF Model for Physically‐based Rendering.pdf;/Users/julianstamm/Zotero/storage/N963KCCM/1467-8659.html}
}

@online{schutte,
  title = {Rendering the {{Moana Island Scene Part}} 1: {{Implementing}} the {{Disney BSDF}}},
  author = {Schutte, Joe},
  url = {https://schuttejoe.github.io/post/disneybsdf/},
  urldate = {2024-12-21},
  organization = {Importance Sampling techniques for GGX with Smith Masking-Shadowing},
  file = {/Users/julianstamm/Zotero/storage/J4RCFEZ4/disneybsdf.html}
}

@online{schutte2018,
  title = {Importance {{Sampling}} Techniques for {{GGX}} with {{Smith Masking-Shadowing}}: {{Part}} 2},
  author = {Schutte, Joe},
  date = {2018-03-07},
  url = {https://schuttejoe.github.io/post/ggximportancesamplingpart2/},
  urldate = {2024-08-28},
  organization = {Importance Sampling techniques for GGX with Smith Masking-Shadowing},
  file = {/Users/julianstamm/Zotero/storage/XWYF4XPM/ggximportancesamplingpart2.html}
}

@incollection{shirley1995,
  title = {Global {{Illumination}} via {{Density-Estimation}}},
  booktitle = {Rendering {{Techniques}} ’95},
  author = {Shirley, Peter and Wade, Bretton and Hubbard, Philip M. and Zareski, David and Walter, Bruce and Greenberg, Donald P.},
  editor = {Hanrahan, Patrick M. and Purgathofer, Werner},
  date = {1995},
  pages = {219--230},
  publisher = {Springer Vienna},
  location = {Vienna},
  doi = {10.1007/978-3-7091-9430-0_21},
  url = {http://link.springer.com/10.1007/978-3-7091-9430-0_21},
  urldate = {2025-08-09},
  isbn = {978-3-211-82733-8 978-3-7091-9430-0},
  langid = {english}
}

@article{sloan2002,
  title = {Precomputed {{Radiance Transfer}} for {{Real-Time Rendering}} in {{Dynamic}}, {{Low-Frequency Lighting Environments}}},
  author = {Sloan, Peter-Pike and Kautz, Jan and Snyder, John},
  editor = {Whitton, Mary C.},
  date = {2002-07-01},
  journaltitle = {ACM Trans. Graph.},
  volume = {21},
  number = {3},
  pages = {527--536},
  issn = {0730-0301},
  doi = {10.1145/566654.566612},
  url = {https://www.ppsloan.org/publications/shillum_final23.pdf},
  urldate = {2025-08-22},
  langid = {english},
  file = {/Users/julianstamm/Zotero/storage/LH3GYUAA/Sloan et al. - Precomputed Radiance Transfer for Real-Time Rendering in Dynamic, Low-Frequency Lighting Environment.pdf;/Users/julianstamm/Zotero/storage/INAPSMPD/47860978_Precomputed_Radiance_Transfer_for_Real-Time_Rendering_in_Dynamic_Low-Frequency_Lightin.html}
}

@incollection{smal2019,
  title = {Real-{{Time Global Illumination}} with {{Photon Mapping}}},
  booktitle = {Ray {{Tracing Gems}}: {{High-Quality}} and {{Real-Time Rendering}} with {{DXR}} and {{Other APIs}}},
  author = {Smal, Niklas and Aizenshtein, Maksim},
  editor = {Haines, Eric and Akenine-Möller, Tomas},
  date = {2019},
  pages = {409--436},
  publisher = {Apress},
  location = {Berkeley, CA},
  doi = {10.1007/978-1-4842-4427-2_24},
  url = {https://doi.org/10.1007/978-1-4842-4427-2_24},
  urldate = {2025-05-18},
  abstract = {Indirect lighting, also known as global illumination, is a crucial effect in photorealistic images. While there are a number of effective global illumination techniques based on precomputation that work well with static scenes, including global illumination for scenes with dynamic lighting and dynamic geometry remains a challenging problem. In this chapter, we describe a real-time global illumination algorithm based on photon mapping that evaluates several bounces of indirect lighting without any precomputed data in scenes with both dynamic lighting and fully dynamic geometry. We explain both the pre- and post-processing steps required to achieve dynamic high-quality illumination within the limits of a realtime frame budget.},
  isbn = {978-1-4842-4427-2},
  langid = {english},
  file = {/Users/julianstamm/Zotero/storage/39CXCRPU/Smal and Aizenshtein - 2019 - Real-Time Global Illumination with Photon Mapping.pdf}
}

@article{smith1967,
  title = {Lunar Surface Roughness: {{Shadowing}} and Thermal Emission},
  shorttitle = {Lunar Surface Roughness},
  author = {Smith, Bruce G.},
  date = {1967-08-15},
  journaltitle = {Journal of Geophysical Research},
  shortjournal = {J. Geophys. Res.},
  volume = {72},
  number = {16},
  pages = {4059--4067},
  issn = {01480227},
  doi = {10.1029/JZ072i016p04059},
  url = {http://doi.wiley.com/10.1029/JZ072i016p04059},
  urldate = {2025-06-24},
  langid = {english},
  file = {/Users/julianstamm/Zotero/storage/KJKUWCU5/Smith - 1967 - Lunar surface roughness Shadowing and thermal emission.pdf}
}

@inproceedings{stachowiak2018,
  title = {Stochastic {{All}} the {{Things}}: {{Raytracing}} in {{Hybrid Real-Time Rendering}}},
  booktitle = {Programming and {{Technology Track}}},
  author = {Stachowiak, Tomasz},
  date = {2018-07-25T20:14:22},
  publisher = {Electronic Arts Inc.},
  url = {https://www.ea.com/seed/news/seed-dd18-presentation-slides-raytracing},
  urldate = {2025-08-24},
  abstract = {Presentation slides \& video from SEED's Digital Dragons 2018 talk, "Stochastic All The Things: Raytracing in Hybrid Real-Time Rendering"},
  eventtitle = {Digital {{Dragons}} 2018},
  langid = {english},
  file = {/Users/julianstamm/Zotero/storage/4MJTSGHU/Electronic Arts - 2018 - Raytracing in Hybrid Real-Time Rendering.pdf;/Users/julianstamm/Zotero/storage/ENCLARMB/seed-dd18-presentation-slides-raytracing.html}
}

@software{stamm2025,
  title = {Julcst/Binrc},
  author = {Stamm, Julian Caspar},
  date = {2025-08-05T18:44:55Z},
  origdate = {2024-10-01T16:08:16Z},
  url = {https://github.com/julcst/binrc}
}

@inproceedings{takikawa2023,
  title = {Compact {{Neural Graphics Primitives}} with {{Learned Hash Probing}}},
  booktitle = {{{SIGGRAPH Asia}} 2023 {{Conference Papers}}},
  author = {Takikawa, Towaki and Müller, Thomas and Nimier-David, Merlin and Evans, Alex and Fidler, Sanja and Jacobson, Alec and Keller, Alexander},
  date = {2023-12-10},
  pages = {1--10},
  publisher = {ACM},
  location = {Sydney NSW Australia},
  doi = {10.1145/3610548.3618167},
  url = {https://dl.acm.org/doi/10.1145/3610548.3618167},
  urldate = {2025-04-22},
  eventtitle = {{{SA}} '23: {{SIGGRAPH Asia}} 2023},
  isbn = {979-8-4007-0315-7},
  langid = {english},
  file = {/Users/julianstamm/Zotero/storage/5PV4YKD5/Takikawa et al. - 2023 - Compact Neural Graphics Primitives with Learned Hash Probing.pdf;/Users/julianstamm/Zotero/storage/FQX3VKFC/Takikawa et al. - 2023 - Compact Neural Graphics Primitives with Learned Hash Probing.pdf}
}

@article{talbot2005,
  title = {Importance {{Resampling}} for {{Global Illumination}}},
  author = {Talbot, Justin F.},
  date = {2005},
  url = {https://scholar.archive.org/work/sxqgndnsanea5ffxwmopgy42uy/access/wayback/https://scholarsarchive.byu.edu/cgi/viewcontent.cgi?article=1662&context=etd},
  urldate = {2025-08-24},
  file = {/Users/julianstamm/Zotero/storage/HF449P6C/Talbot - 2005 - Importance Resampling for Global Illumination.pdf}
}

@article{tancik2020,
  title = {Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains},
  author = {Tancik, Matthew and Srinivasan, Pratul and Mildenhall, Ben and Fridovich-Keil, Sara and Raghavan, Nithin and Singhal, Utkarsh and Ramamoorthi, Ravi and Barron, Jonathan and Ng, Ren},
  date = {2020},
  journaltitle = {Advances in neural information processing systems},
  volume = {33},
  pages = {7537--7547},
  url = {https://proceedings.neurips.cc/paper_files/paper/2020/hash/55053683268957697aa39fba6f231c68-Abstract.html},
  urldate = {2025-07-07},
  file = {/Users/julianstamm/Zotero/storage/AP9ISDGE/Tancik et al. - 2020 - Fourier features let networks learn high frequency functions in low dimensional domains.pdf}
}

@inproceedings{tatarchuk2005,
  title = {Irradiance Volumes for Games},
  booktitle = {Game {{Developer}}’s {{Conference}}},
  author = {Tatarchuk, Natalya},
  date = {2005},
  volume = {3},
  publisher = {Citeseer},
  url = {https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=4712b768b044da8ab21e6398af43a3eb7f836a59},
  urldate = {2025-08-22},
  file = {/Users/julianstamm/Zotero/storage/QM6A57ZK/Tatarchuk - 2005 - Irradiance volumes for games.pdf}
}

@article{tatzgern2024,
  title = {Radiance {{Caching}} with {{On-Surface Caches}} for {{Real-Time Global Illumination}}},
  author = {Tatzgern, Wolfgang and Weinrauch, Alexander and Stadlbauer, Pascal and Mueller, Joerg H. and Winter, Martin and Steinberger, Markus},
  date = {2024-08-09},
  journaltitle = {Proceedings of the ACM on Computer Graphics and Interactive Techniques},
  shortjournal = {Proc. ACM Comput. Graph. Interact. Tech.},
  volume = {7},
  number = {3},
  pages = {1--17},
  publisher = {ACMPUB27New York, NY, USA},
  issn = {2577-6193},
  doi = {10.1145/3675382},
  url = {https://dl.acm.org/doi/10.1145/3675382},
  urldate = {2025-06-28},
  abstract = {Achieving global illumination is crucial for delivering realistic lighting in real-time rendering applications. Despite recent advancements in hardware ray tracing, the computational demands of full path tracing remain largely impractical for real-world production scenarios. We introduce a novel two-level radiance caching system that exclusively utilizes on-surface caches, diverging from conventional approaches that combine screen-space and world-space caches. Unlike previous texture space techniques, which mostly prioritize closely matching the resolution to screen space to minimize artifacts, our focus is on achieving optimal visual quality with minimal texture space resolutions. By caching directional radiance information on both primary and secondary hits, our approach delivers high-quality renderings of global illumination while being computationally efficient. Overall, this leads to an up to 5\% to 10\% improvement in both speed and quality compared to other state-of-the-art approaches. Our proposed method is versatile, handling not only diffuse global illumination but also addressing (glossy) reflections. Furthermore, our approach is well-suited for multi-viewer rendering, as the utilization of on-surface caches enables information sharing among different viewers, making it applicable to cloud-native rendering environments.},
  langid = {english},
  file = {/Users/julianstamm/Zotero/storage/RV589SBU/Tatzgern et al. - 2024 - Radiance Caching with On-Surface Caches for Real-Time Global Illumination.pdf;/Users/julianstamm/Zotero/storage/YV2SPHGK/Tatzgern et al. - 2024 - Radiance Caching with On-Surface Caches for Real-Time Global Illumination.pdf;/Users/julianstamm/Zotero/storage/4P8J5QL8/3675382.html}
}

@software{tensorflowdevelopers2021,
  title = {{{TensorFlow}}},
  author = {{TensorFlow Developers}},
  date = {2021-05},
  doi = {10.5281/zenodo.4758419},
  url = {https://doi.org/10.5281/zenodo.4758419},
  organization = {Zenodo},
  version = {v2.5.0}
}

@report{thekhronosr3dformatsworkinggroup2021,
  title = {{{glTF}}™  2.0 {{Specification}}},
  author = {{The Khronos® 3D Formats Working Group}},
  date = {2021-10-11},
  url = {https://registry.khronos.org/glTF/specs/2.0/glTF-2.0.html}
}

@online{thomas2018,
  title = {Deep {{Illumination}}: {{Approximating Dynamic Global Illumination}} with {{Generative Adversarial Network}}},
  shorttitle = {Deep {{Illumination}}},
  author = {Thomas, Manu Mathew and Forbes, Angus G.},
  date = {2018-05-22},
  eprint = {1710.09834},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1710.09834},
  url = {http://arxiv.org/abs/1710.09834},
  urldate = {2025-07-29},
  abstract = {We present Deep Illumination, a novel machine learning technique for approximating global illumination (GI) in real-time applications using a Conditional Generative Adversarial Network. Our primary focus is on generating indirect illumination and soft shadows with offline rendering quality at interactive rates. Inspired from recent advancement in image-to-image translation problems using deep generative convolutional networks, we introduce a variant of this network that learns a mapping from Gbuffers (depth map, normal map, and diffuse map) and direct illumination to any global illumination solution. Our primary contribution is showing that a generative model can be used to learn a density estimation from screen space buffers to an advanced illumination model for a 3D environment. Once trained, our network can approximate global illumination for scene configurations it has never encountered before within the environment it was trained on. We evaluate Deep Illumination through a comparison with both a state of the art real-time GI technique (VXGI) and an offline rendering GI technique (path tracing). We show that our method produces effective GI approximations and is also computationally cheaper than existing GI techniques. Our technique has the potential to replace existing precomputed and screen-space techniques for producing global illumination effects in dynamic scenes with physically-based rendering quality.},
  pubstate = {prepublished},
  keywords = {Computer Science - Graphics},
  file = {/Users/julianstamm/Zotero/storage/Q54ZIXM9/Thomas and Forbes - 2018 - Deep Illumination Approximating Dynamic Global Illumination with Generative Adversarial Network.pdf}
}

@article{tole2002,
  title = {Interactive Global Illumination in Dynamic Scenes},
  author = {Tole, Parag and Pellacini, Fabio and Walter, Bruce and Greenberg, Donald P.},
  date = {2002-07},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {21},
  number = {3},
  pages = {537--546},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/566654.566613},
  url = {https://dl.acm.org/doi/10.1145/566654.566613},
  urldate = {2025-08-22},
  abstract = {In this paper, we present a system for interactive computation of global illumination in dynamic scenes. Our system uses a novel scheme for caching the results of a high quality pixel-based renderer such as a bidirectional path tracer. The Shading Cache is an object-space hierarchical subdivision mesh with lazily computed shading values at its vertices. A high frame rate display is generated from the Shading Cache using hardware-based interpolation and texture mapping. An image space sampling scheme refines the Shading Cache in regions that have the most interpolation error or those that are most likely to be affected by object or camera motion.Our system handles dynamic scenes and moving light sources efficiently, providing useful feedback within a few seconds and high quality images within a few tens of seconds, without the need for any pre-computation. Our approach allows us to significantly outperform other interactive systems based on caching ray-tracing samples, especially in dynamic scenes. Based on our results, we believe that the Shading Cache will be an invaluable tool in lighting design and modelling while rendering.},
  langid = {english},
  file = {/Users/julianstamm/Zotero/storage/RXF8NB2D/Tole et al. - 2002 - Interactive global illumination in dynamic scenes.pdf}
}

@article{torrance1967,
  title = {Theory for {{Off-Specular Reflection From Roughened Surfaces}}*},
  author = {Torrance, Kenneth E. and Sparrow, Ephraim M.},
  date = {1967-09-01},
  journaltitle = {Journal of the Optical Society of America},
  shortjournal = {J. Opt. Soc. Am.},
  volume = {57},
  number = {9},
  pages = {1105--1114},
  publisher = {Optical Society of America},
  issn = {0030-3941},
  doi = {10.1364/JOSA.57.001105},
  url = {https://opg.optica.org/abstract.cfm?URI=josa-57-9-1105},
  urldate = {2025-06-25},
  langid = {english},
  file = {/Users/julianstamm/Zotero/storage/HNTX7UJJ/Torrance and Sparrow - 1967 - Theory for Off-Specular Reflection From Roughened Surfaces.pdf}
}

@article{trowbridge1975,
  title = {Average Irregularity Representation of a Rough Surface for Ray Reflection},
  author = {Trowbridge, T. S. and Reitz, K. P.},
  date = {1975-05-01},
  journaltitle = {Journal of the Optical Society of America},
  shortjournal = {J. Opt. Soc. Am.},
  volume = {65},
  number = {5},
  pages = {531},
  issn = {0030-3941},
  doi = {10.1364/JOSA.65.000531},
  url = {https://opg.optica.org/abstract.cfm?URI=josa-65-5-531},
  urldate = {2024-03-09},
  langid = {english}
}

@online{typesraytracing,
  title = {Types of {{Ray Tracing}}},
  url = {https://cs.stanford.edu/people/eroberts/courses/soco/projects/1997-98/ray-tracing/types.html},
  urldate = {2025-04-24},
  file = {/Users/julianstamm/Zotero/storage/LQ5M3P3Z/types.html}
}

@article{vaswani2017,
  title = {Attention Is All You Need},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Łukasz and Polosukhin, Illia},
  date = {2017},
  journaltitle = {Advances in neural information processing systems},
  volume = {30},
  url = {https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
  urldate = {2025-07-08},
  file = {/Users/julianstamm/Zotero/storage/9MTXSP7E/Vaswani et al. - 2017 - Attention is all you need.pdf}
}

@incollection{veach1995,
  title = {Bidirectional {{Estimators}} for {{Light Transport}}},
  booktitle = {Photorealistic {{Rendering Techniques}}},
  author = {Veach, Eric and Guibas, Leonidas},
  editor = {Sakas, Georgios and Müller, Stefan and Shirley, Peter},
  date = {1995},
  pages = {145--167},
  publisher = {Springer Berlin Heidelberg},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-87825-1_11},
  url = {http://link.springer.com/10.1007/978-3-642-87825-1_11},
  urldate = {2025-04-24},
  isbn = {978-3-642-87827-5 978-3-642-87825-1},
  langid = {english},
  file = {/Users/julianstamm/Zotero/storage/M97AIECC/Veach94.pdf}
}

@thesis{veach1997,
  title = {Robust {{Monte Carlo}} Methods for Light Transport Simulation},
  author = {Veach, Eric},
  date = {1997-12},
  institution = {Stanford University},
  url = {https://graphics.stanford.edu/papers/veach_thesis/thesis.pdf},
  urldate = {2024-12-21},
  file = {/Users/julianstamm/Zotero/storage/5PLIJRDX/veach1997.pdf}
}

@inproceedings{veach1997a,
  title = {Metropolis {{Light Transport}}},
  booktitle = {Proceedings of the 24th Annual Conference on {{Computer}} Graphics and Interactive Techniques  - {{SIGGRAPH}} '97},
  author = {Veach, Eric and Guibas, Leonidas J.},
  date = {1997},
  pages = {65--76},
  publisher = {ACM Press},
  location = {Not Known},
  doi = {10.1145/258734.258775},
  url = {http://portal.acm.org/citation.cfm?doid=258734.258775},
  urldate = {2025-08-21},
  eventtitle = {The 24th Annual Conference},
  isbn = {978-0-89791-896-1},
  langid = {english},
  file = {/Users/julianstamm/Zotero/storage/N2US7DQX/Veach and Guibas - 1997 - Metropolis light transport.pdf}
}

@article{vorba2011,
  title = {Bidirectional {{Photon Mapping}}},
  author = {Vorba, Jiˇrı},
  date = {2011},
  abstract = {This paper introduces a method for optimal combination of light paths generated from the camera and from the light sources in the photon mapping algorithm used for computing global illumination. Our method is based on Multiple Importance Sampling, a general approach, introduced by Veach, for adaptive path connection in bi-directional pathtracing. Our goal is to examine this method in connection with the biased algorithm of photon mapping and to improve the ineffective final gather heuristic used in the original version of this algorithm. This heuristic is usually problematic when applied to the scenes where highly glossy materials prevail.},
  langid = {english},
  file = {/Users/julianstamm/Zotero/storage/T4U58Q9V/Vorba - Bidirectional Photon Mapping.pdf}
}

@article{vorba2014,
  title = {On-Line Learning of Parametric Mixture Models for Light Transport Simulation},
  author = {Vorba, Jiří and Karlík, Ondřej and Šik, Martin and Ritschel, Tobias and Křivánek, Jaroslav},
  date = {2014-07-27},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {33},
  number = {4},
  pages = {101:1--101:11},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/2601097.2601203},
  url = {https://dl.acm.org/doi/10.1145/2601097.2601203},
  urldate = {2025-08-21},
  abstract = {Monte Carlo techniques for light transport simulation rely on importance sampling when constructing light transport paths. Previous work has shown that suitable sampling distributions can be recovered from particles distributed in the scene prior to rendering. We propose to represent the distributions by a parametric mixture model trained in an on-line (i.e. progressive) manner from a potentially infinite stream of particles. This enables recovering good sampling distributions in scenes with complex lighting, where the necessary number of particles may exceed available memory. Using these distributions for sampling scattering directions and light emission significantly improves the performance of state-of-the-art light transport simulation algorithms when dealing with complex lighting.},
  langid = {english},
  file = {/Users/julianstamm/Zotero/storage/JFHPG88Y/Vorba et al. - 2014 - On-line learning of parametric mixture models for light transport simulation.pdf}
}

@article{vorba2014a,
  title = {On-Line {{Learning}} of {{Parametric Mixture Models}} for {{Light Transport Simulation}}–{{Supplemental}} Material},
  author = {Vorba, Jirı and Karlık, Ondrej and Šik, Martin and Ritschel, Tobias and Krivánek, Jaroslav},
  date = {2014},
  url = {https://cgg.mff.cuni.cz/~jaroslav/papers/2014-onlineis/2014-onlineis-supplemental_doc.pdf},
  urldate = {2025-08-21},
  file = {/Users/julianstamm/Zotero/storage/W7RXDZ5I/Vorba et al. - On-line Learning of Parametric Mixture Models for Light Transport Simulation–Supplemental material.pdf}
}

@article{walter1997,
  title = {Global {{Illumination Using Local Linear Density Estimation}}},
  author = {Walter, Bruce and Hubbard, Philip M. and Shirley, Peter and Greenberg, Donald P.},
  date = {1997-07},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {16},
  number = {3},
  pages = {217--259},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/256157.256158},
  url = {https://dl.acm.org/doi/10.1145/256157.256158},
  urldate = {2025-08-09},
  abstract = {This article presents the density estimation framework for generating view-independent global illumination solutions. It works by probabilistically simulating the light flow in an environment with light particles that trace random walks origination at luminaires and then using statistical density estimation techniques to reconstruct the lighting on each surface. By splitting the computation into separate transport and reconstruction stages, we gain many advantages including reduced memory usage, the ability to simulate nondiffuse transport, and natural parallelism. Solutions to several theoretical and practical difficulties in implementing this framework are also described. Light sources that vary spectrally and directionally are integrated into a spectral particle tracer using  nonuniform rejection. A new local linear density estimation technique eliminates boundary bias and extends to arbitrary polygons. A mesh decimation algorithm with perceptual calibration is introduced to simplify the Gouraud-shaded representation of the solution for interactive display.},
  langid = {english},
  file = {/Users/julianstamm/Zotero/storage/7ZR2LPFA/Walter et al. - 1997 - Global illumination using local linear density estimation.pdf}
}

@incollection{walter1999,
  title = {Interactive {{Rendering}} Using the {{Render Cache}}},
  booktitle = {Rendering {{Techniques}}’ 99},
  author = {Walter, Bruce and Drettakis, George and Parker, Steven},
  editor = {Lischinski, Dani and Larson, Greg Ward},
  date = {1999},
  pages = {19--30},
  publisher = {Springer Vienna},
  location = {Vienna},
  doi = {10.1007/978-3-7091-6809-7_3},
  url = {http://link.springer.com/10.1007/978-3-7091-6809-7_3},
  urldate = {2025-08-22},
  isbn = {978-3-211-83382-7 978-3-7091-6809-7},
  langid = {english},
  file = {/Users/julianstamm/Zotero/storage/5M8H3J24/Walter et al. - 1999 - Interactive Rendering using the Render Cache.pdf}
}

@article{walter2007,
  title = {Microfacet {{Models}} for {{Refraction}} through {{Rough Surfaces}}.},
  author = {Walter, Bruce and Marschner, Stephen R. and Li, Hongsong and Torrance, Kenneth E.},
  date = {2007},
  journaltitle = {Rendering techniques},
  volume = {2007},
  pages = {18th},
  url = {https://www.graphics.cornell.edu/~bjw/microfacetbsdf.pdf},
  urldate = {2025-06-25},
  file = {/Users/julianstamm/Zotero/storage/AJEVMZ48/Walter et al. - Microfacet Models for Refraction through Rough Surfaces.pdf;/Users/julianstamm/Zotero/storage/BCWPRGM3/Walter et al. - 2007 - Microfacet Models for Refraction through Rough Surfaces..pdf}
}

@article{ward1988,
  title = {A Ray Tracing Solution for Diffuse Interreflection},
  author = {Ward, Gregory J. and Rubinstein, Francis M. and Clear, Robert D.},
  date = {1988-08},
  journaltitle = {ACM SIGGRAPH Computer Graphics},
  shortjournal = {SIGGRAPH Comput. Graph.},
  volume = {22},
  number = {4},
  pages = {85--92},
  issn = {0097-8930},
  doi = {10.1145/378456.378490},
  url = {https://dl.acm.org/doi/10.1145/378456.378490},
  urldate = {2025-04-22},
  abstract = {An efficient ray tracing method is presented for calculating interreflections between surfaces with both diffuse and specular components. A Monte Carlo technique computes the indirect contributions to illuminance at locations chosen by the rendering process. The indirect illuminance values are averaged over surfaces and used in place of a constant "ambient" term. Illuminance calculations are made only for those areas participating in the selected view, and the results are stored so that subsequent views can reuse common values. The density of the calculation is adjusted to maintain a constant accuracy, permitting less populated portions of the scene to be computed quickly. Successive reflections use proportionally fewer samples, which speeds the process and provides a natural limit to recursion. The technique can also model diffuse transmission and illumination from large area sources, such as the sky.},
  langid = {english},
  file = {/Users/julianstamm/Zotero/storage/X22JZBZ4/Ward et al. - 1988 - A ray tracing solution for diffuse interreflection.pdf}
}

@article{whitted1980,
  title = {An {{Improved Illumination Model}} for {{Shaded Display}}},
  author = {Whitted, Turner},
  date = {1980},
  journaltitle = {Communications of the ACM},
  volume = {23},
  number = {6},
  pages = {343--349},
  publisher = {Association for Computing Machinery (ACM)},
  doi = {10.1145/1198555.1198743},
  url = {https://cir.nii.ac.jp/crid/1362544420076304768},
  urldate = {2025-08-11},
  langid = {english},
  file = {/Users/julianstamm/Zotero/storage/H2AFXBRI/Whitted - 1980 - An Improved Illumination Model for Shaded Display.pdf}
}

@article{whitted2020,
  title = {Origins of {{Global Illumination}}},
  author = {Whitted, Turner},
  date = {2020-01-01},
  journaltitle = {IEEE Computer Graphics and Applications},
  shortjournal = {IEEE Comput. Grap. Appl.},
  volume = {40},
  number = {1},
  pages = {20--27},
  issn = {0272-1716, 1558-1756},
  doi = {10.1109/MCG.2019.2957688},
  url = {https://ieeexplore.ieee.org/document/8951772/},
  urldate = {2025-08-11},
  abstract = {Global illumination refers to a complete shading model that simulates real lighting and reflection as accurately as possible. Whether used for product prototyping or special effects for entertainment, the goal is to match the appearance of the real world. The origins of global illumination come at the intersection of a steady progression of shading models with the ancient simulation technique of ray tracing.},
  langid = {english},
  file = {/Users/julianstamm/Zotero/storage/2GGLRY7P/Whitted - 2020 - Origins of Global Illumination.pdf}
}

@article{wright2021,
  title = {Radiance {{Caching}} for {{Real-Time Global Illumination}}},
  author = {Wright, Daniel},
  date = {2021},
  journaltitle = {ACM SIGGRAPH 2021 Advances in Real-Time Rendering in Game Course},
  pages = {9--13},
  url = {https://advances.realtimerendering.com/s2021/index.html},
  abstract = {This talk will present an efficient and high-quality Final Gather for fully dynamic Global Illumination with ray tracing, targeted at next generation consoles and shipping in Unreal Engine 5.  Hardware Ray Tracing provides a new and powerful tool for real-time graphics, but current hardware can barely afford 1 ray per pixel for diffuse indirect, while Global Illumination needs hundreds of effective samples for high quality indoor lighting.  Existing approaches that rely on Irradiance Fields cannot scale up in quality, while approaches relying on a Screen Space denoiser have exorbitant costs at high resolutions.  This talk will present practical applications of Radiance Caching along with effective techniques to reduce noise and leaking.},
  file = {/Users/julianstamm/Zotero/storage/HWF7FPS6/Wright - 2021 - Radiance Caching for Real-Time Global Illumination.pdf}
}

@inproceedings{wright2022,
  title = {Real-Time {{Global Illumination}} in {{Unreal Engine}} 5},
  booktitle = {{{SIGGRAPH}} 2022 {{Advances}} in {{Real-Time Rendering}} in {{Games}} Course},
  author = {Wright, Daniel and Narkowicz, Krzysztof and Kelly, Patrick},
  date = {2022-08},
  location = {Vancouver},
  url = {https://advances.realtimerendering.com/s2022/SIGGRAPH2022-Advances-Lumen-Wright%20et%20al.pdf},
  urldate = {2025-06-28},
  eventtitle = {{{SIGGRAPH}} 2022},
  file = {/Users/julianstamm/Zotero/storage/BVIGP7Z3/Lumen Siggraph 2022.pdf}
}

@online{wu2024,
  title = {Neural {{Directional Encoding}} for {{Efficient}} and {{Accurate View-Dependent Appearance Modeling}}},
  author = {Wu, Liwen and Bi, Sai and Xu, Zexiang and Luan, Fujun and Zhang, Kai and Georgiev, Iliyan and Sunkavalli, Kalyan and Ramamoorthi, Ravi},
  date = {2024-05-23},
  eprint = {2405.14847},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2405.14847},
  url = {http://arxiv.org/abs/2405.14847},
  urldate = {2025-06-29},
  abstract = {Novel-view synthesis of specular objects like shiny metals or glossy paints remains a significant challenge. Not only the glossy appearance but also global illumination effects, including reflections of other objects in the environment, are critical components to faithfully reproduce a scene. In this paper, we present Neural Directional Encoding (NDE), a view-dependent appearance encoding of neural radiance fields (NeRF) for rendering specular objects. NDE transfers the concept of feature-grid-based spatial encoding to the angular domain, significantly improving the ability to model high-frequency angular signals. In contrast to previous methods that use encoding functions with only angular input, we additionally cone-trace spatial features to obtain a spatially varying directional encoding, which addresses the challenging interreflection effects. Extensive experiments on both synthetic and real datasets show that a NeRF model with NDE (1) outperforms the state of the art on view synthesis of specular objects, and (2) works with small networks to allow fast (real-time) inference. The project webpage and source code are available at: \textbackslash url\{https://lwwu2.github.io/nde/\}.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/julianstamm/Zotero/storage/BZK7HF7I/Wu et al. - 2024 - Neural Directional Encoding for Efficient and Accurate View-Dependent Appearance Modeling.pdf}
}

@inproceedings{wyman2023,
  title = {A {{Gentle Introduction}} to {{ReSTIR Path Reuse}} in {{Real-Time}}},
  booktitle = {{{ACM SIGGRAPH}} 2023 {{Courses}}},
  author = {Wyman, Chris and Kettunen, Markus and Lin, Daqi and Bitterli, Benedikt and Yuksel, Cem and Jarosz, Wojciech and Kozlowski, Pawel},
  date = {2023-07-24},
  series = {{{SIGGRAPH}} '23},
  pages = {1--38},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3587423.3595511},
  url = {https://dl.acm.org/doi/10.1145/3587423.3595511},
  urldate = {2025-07-11},
  abstract = {In recent years, reservoir-based spatiotemporal importance resampling (ReSTIR) algorithms appeared out of nowhere to take parts of the realtime rendering community by storm, with sample reuse speeding direct lighting from millions of dynamic lights [1], diffuse multi-bounce lighting [2], participating media [3], and even complex global illumination paths [4]. Highly optimized variants (e.g. [5]) can give 100x efficiency improvement over traditional ray- and path-tracing methods; this is key to achieve 30 or 60 Hz framerates. In production engines, tracing even one ray or path per pixel may only be feasible on the highest-end systems, so maximizing image quality per sample is vital.ReSTIR builds on the math in Talbot et al.'s [6] resampled importance sampling (RIS), which previously was not widely used or taught, leaving many practitioners missing key intuitions and theoretical grounding. A firm grounding is vital, as seemingly obvious "optimizations" arising during ReSTIR engine integration can silently introduce conditional probabilities and dependencies that, left ignored, add uncontrollable bias to the results.In this course, we plan to:1. Provide concrete motivation and intuition for why ReSTIR works, where it applies, what assumptions it makes, and the limitations of today's theory and implementations;2. Gently develop the theory, targeting attendees with basic Monte Carlo sampling experience but without prior knowledge of resampling algorithms (e.g., Talbot et al. [6]);3. Give explicit algorithmic samples and pseudocode, pointing out easily-encountered pitfalls when implementing ReSTIR;4. Discuss actual game integrations, highlighting the gotchas, challenges, and corner cases we encountered along the way, and highlighting ReSTIR's practical benefits.},
  isbn = {979-8-4007-0145-0},
  file = {/Users/julianstamm/Zotero/storage/34Q29SU3/2023ReSTIR_Course_DirectLight_and_Spatiotemporal.pdf;/Users/julianstamm/Zotero/storage/9PEEPS62/2023ReSTIR_Course_Cyberpunk_2077_Integration.pdf;/Users/julianstamm/Zotero/storage/EPC8JZR4/Wyman et al. - 2023 - A Gentle Introduction to ReSTIR Path Reuse in Real-Time.pdf;/Users/julianstamm/Zotero/storage/FIS8FF2B/2023ReSTIR_Course_AlgorithmicOptimizations.pdf;/Users/julianstamm/Zotero/storage/NT9KC9RH/2023ReSTIR_Course_RIS.pdf;/Users/julianstamm/Zotero/storage/Q7LW9NG7/2023ReSTIR_Course_Welcome.pdf;/Users/julianstamm/Zotero/storage/UHGZYUM3/2023ReSTIR_Course_LowLevelOptimizations.pdf;/Users/julianstamm/Zotero/storage/VMBZT6TA/214-restir.mp4;/Users/julianstamm/Zotero/storage/WSEA5CRA/2023ReSTIR_Course_ExtendingReuse.pdf;/Users/julianstamm/Zotero/storage/ZBKR6PHG/2023ReSTIR_Course_Preliminaries.pdf;/Users/julianstamm/Zotero/storage/ZSALJTKX/2023ReSTIR_Course_ReuseBetweenDomains.pdf}
}

@article{xie2022,
  title = {Neural {{Fields}} in {{Visual Computing}} and {{Beyond}}},
  author = {Xie, Yiheng and Takikawa, Towaki and Saito, Shunsuke and Litany, Or and Yan, Shiqin and Khan, Numair and Tombari, Federico and Tompkin, James and Sitzmann, Vincent and Sridhar, Srinath},
  date = {2022-05},
  journaltitle = {Computer Graphics Forum},
  shortjournal = {Computer Graphics Forum},
  volume = {41},
  number = {2},
  pages = {641--676},
  issn = {0167-7055, 1467-8659},
  doi = {10.1111/cgf.14505},
  url = {https://onlinelibrary.wiley.com/doi/10.1111/cgf.14505},
  urldate = {2025-08-25},
  abstract = {Abstract                            Recent advances in machine learning have led to increased interest in solving visual computing problems using methods that employ coordinate‐based neural networks. These methods, which we call               neural fields               , parameterize physical properties of scenes or objects across space and time. They have seen widespread success in problems such as 3D shape and image synthesis, animation of human bodies, 3D reconstruction, and pose estimation. Rapid progress has led to numerous papers, but a consolidation of the discovered knowledge has not yet emerged. We provide context, mathematical grounding, and a review of over 250 papers in the literature on neural fields. In               Part I               , we focus on neural field techniques by identifying common components of neural field methods, including different conditioning, representation, forward map, architecture, and manipulation methods. In               Part II               , we focus on applications of neural fields to different problems in visual computing, and beyond (e.g., robotics, audio). Our review shows the breadth of topics already covered in visual computing, both historically and in current incarnations, and highlights the improved quality, flexibility, and capability brought by neural field methods. Finally, we present a companion website that acts as a living database that can be continually updated by the community.},
  langid = {english},
  file = {/Users/julianstamm/Zotero/storage/F2Y5SDU3/Xie et al. - 2022 - Neural Fields in Visual Computing and Beyond.pdf}
}

@article{xing2024,
  title = {Real-Time All-Frequency Global Illumination with Radiance Caching},
  author = {Xing, Youxin and Pan, Gaole and Chen, Xiang and Wu, Ji and Wang, Lu and Wang, Beibei},
  date = {2024-10},
  journaltitle = {Computational Visual Media},
  volume = {10},
  number = {5},
  pages = {923--936},
  publisher = {TUP},
  issn = {2096-0662},
  doi = {10.1007/s41095-023-0367-z},
  url = {https://ieeexplore.ieee.org/abstract/document/10885008},
  urldate = {2025-08-23},
  abstract = {Global illumination (GI) plays a crucial role in rendering realistic results for virtual exhibitions, such as virtual car exhibitions. These scenarios usually include all-frequency bidirectional reflectance distribution functions (BRDFs), although their geometries and light configurations may be static. Rendering all-frequency BRDFs in real time remains challenging due to the complex light transport. Existing approaches, including precomputed radiance transfer, light probes, and the most recent path-tracing-based approaches (ReSTIR PT), cannot satisfy both quality and performance requirements simultaneously. Herein, we propose a practical hybrid global illumination approach that combines ray tracing and cached GI by caching the incoming radiance with wavelets. Our approach can produce results close to those of offline renderers at the cost of only approximately 17 ms at runtime and is robust over all-frequency BRDFs. Our approach is designed for applications involving static lighting and geometries, such as virtual exhibitions.},
  keywords = {all-frequency BRDFs,Convolution,Geometry,Haar wavelets,Lighting,Octrees,Point cloud compression,radiance caching,Ray tracing,real-time global illumination,Real-time systems,Rendering (computer graphics),Runtime,Vectors},
  file = {/Users/julianstamm/Zotero/storage/BSZP3SIQ/Xing et al. - 2024 - Real-time all-frequency global illumination with radiance caching.pdf}
}

@inproceedings{zheng2024,
  title = {Neural {{Global Illumination}} via {{Superposed Deformable Feature Fields}}},
  booktitle = {{{SIGGRAPH Asia}} 2024 {{Conference Papers}}},
  author = {Zheng, Chuankun and Huo, Yuchi and Huang, Hongxiang and Sheng, Hongtao and Huang, Junrong and Tang, Rui and Zhu, Hao and Wang, Rui and Bao, Hujun},
  date = {2024-12-03},
  series = {{{SA}} '24},
  pages = {1--11},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3680528.3687680},
  url = {https://doi.org/10.1145/3680528.3687680},
  urldate = {2025-05-04},
  abstract = {Interactive rendering of dynamic scenes with complex global illumination has been a long-standing problem in computer graphics. Recent advances in neural rendering demonstrate new promising possibilities. However, while existing methods have achieved impressive results, complex rendering effects (e.g., caustics) remain challenging. This paper presents a novel neural rendering method that is able to generate high-quality global illumination effects, including but not limited to caustics, soft shadows, and indirect highlights, for dynamic scenes with varying camera, lighting conditions, materials, and object transformations. Inspired by object-oriented transfer field representations, we employ deformable neural feature fields to implicitly model the impacts of individual objects or light sources on global illumination. By employing neural feature fields, our method gains the ability to represent high-frequency details, thus supporting complex rendering effects. We superpose these feature fields in latent space and utilize a lightweight decoder to obtain global illumination estimates, which allows our neural representations to spontaneously adapt to the contribution of individual objects or light sources to global illumination in a data-driven manner, thus further improving the quality. Our experiments demonstrate the effectiveness of our method on a wide range of scenes with complex light paths, materials, and geometries.},
  isbn = {979-8-4007-1131-2}
}

@article{zhou2008,
  title = {Real-Time {{KD-tree}} Construction on Graphics Hardware},
  author = {Zhou, Kun and Hou, Qiming and Wang, Rui and Guo, Baining},
  date = {2008-12-01},
  journaltitle = {ACM Trans. Graph.},
  volume = {27},
  number = {5},
  pages = {126:1--126:11},
  issn = {0730-0301},
  doi = {10.1145/1409060.1409079},
  url = {https://doi.org/10.1145/1409060.1409079},
  urldate = {2025-05-13},
  abstract = {We present an algorithm for constructing kd-trees on GPUs. This algorithm achieves real-time performance by exploiting the GPU's streaming architecture at all stages of kd-tree construction. Unlike previous parallel kd-tree algorithms, our method builds tree nodes completely in BFS (breadth-first search) order. We also develop a special strategy for large nodes at upper tree levels so as to further exploit the fine-grained parallelism of GPUs. For these nodes, we parallelize the computation over all geometric primitives instead of nodes at each level. Finally, in order to maintain kd-tree quality, we introduce novel schemes for fast evaluation of node split costs.As far as we know, ours is the first real-time kd-tree algorithm on the GPU. The kd-trees built by our algorithm are of comparable quality as those constructed by off-line CPU algorithms. In terms of speed, our algorithm is significantly faster than well-optimized single-core CPU algorithms and competitive with multi-core CPU algorithms. Our algorithm provides a general way for handling dynamic scenes on the GPU. We demonstrate the potential of our algorithm in applications involving dynamic scenes, including GPU ray tracing, interactive photon mapping, and point cloud modeling.}
}

@online{zhu2020,
  title = {Deep {{Photon Mapping}}},
  author = {Zhu, Shilin and Xu, Zexiang and Jensen, Henrik Wann and Su, Hao and Ramamoorthi, Ravi},
  date = {2020-04-25},
  eprint = {2004.12069},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2004.12069},
  url = {http://arxiv.org/abs/2004.12069},
  urldate = {2025-04-30},
  abstract = {Recently, deep learning-based denoising approaches have led to dramatic improvements in low sample-count Monte Carlo rendering. These approaches are aimed at path tracing, which is not ideal for simulating challenging light transport effects like caustics, where photon mapping is the method of choice. However, photon mapping requires very large numbers of traced photons to achieve high-quality reconstructions. In this paper, we develop the first deep learning-based method for particle-based rendering, and specifically focus on photon density estimation, the core of all particle-based methods. We train a novel deep neural network to predict a kernel function to aggregate photon contributions at shading points. Our network encodes individual photons into per-photon features, aggregates them in the neighborhood of a shading point to construct a photon local context vector, and infers a kernel function from the per-photon and photon local context features. This network is easy to incorporate in many previous photon mapping methods (by simply swapping the kernel density estimator) and can produce high-quality reconstructions of complex global illumination effects like caustics with an order of magnitude fewer photons compared to previous photon mapping methods.},
  pubstate = {prepublished},
  keywords = {Computer Science - Graphics,Computer Science - Machine Learning},
  file = {/Users/julianstamm/Zotero/storage/7L3ATJ4C/Zhu et al. - 2020 - Deep Photon Mapping.pdf;/Users/julianstamm/Zotero/storage/YER4TY8K/2004.html}
}

@online{zhu2022a,
  title = {{{RTNN}}: {{Accelerating Neighbor Search Using Hardware Ray Tracing}}},
  shorttitle = {{{RTNN}}},
  author = {Zhu, Yuhao},
  date = {2022-03-09},
  eprint = {2201.01366},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2201.01366},
  url = {http://arxiv.org/abs/2201.01366},
  urldate = {2025-05-30},
  abstract = {Neighbor search is of fundamental important to many engineering and science fields such as physics simulation and computer graphics. This paper proposes to formulate neighbor search as a ray tracing problem and leverage the dedicated ray tracing hardware in recent GPUs for acceleration. We show that a naive mapping under-exploits the ray tracing hardware. We propose two performance optimizations, query scheduling and query partitioning, to tame the inefficiencies. Experimental results show 2.2X -- 65.0X speedups over existing neighbor search libraries on GPUs. The code is available at https://github.com/horizon-research/rtnn.},
  pubstate = {prepublished},
  keywords = {Computer Science - Distributed Parallel and Cluster Computing,Computer Science - Graphics},
  file = {/Users/julianstamm/Zotero/storage/8ZNSAYGQ/Zhu - 2022 - RTNN Accelerating Neighbor Search Using Hardware Ray Tracing.pdf;/Users/julianstamm/Zotero/storage/LI67UZPS/2201.html}
}

@article{zhu2022b,
  title = {Photon-{{Driven Neural Reconstruction}} for {{Path Guiding}}},
  author = {Zhu, Shilin and Xu, Zexiang and Sun, Tiancheng and Kuznetsov, Alexandr and Meyer, Mark and Jensen, Henrik Wann and Su, Hao and Ramamoorthi, Ravi},
  date = {2022-02-28},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {41},
  number = {1},
  pages = {1--15},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/3476828},
  url = {https://dl.acm.org/doi/10.1145/3476828},
  urldate = {2025-08-25},
  abstract = {Although Monte Carlo path tracing is a simple and effective algorithm to synthesize photo-realistic images, it is often very slow to converge to noise-free results when involving complex global illumination. One of the most successful variance-reduction techniques is path guiding, which can learn better distributions for importance sampling to reduce pixel noise. However, previous methods require a large number of path samples to achieve reliable path guiding. We present a novel neural path guiding approach that can reconstruct high-quality sampling distributions for path guiding from a sparse set of samples, using an offline trained neural network. We leverage photons traced from light sources as the primary input for sampling density reconstruction, which is effective for challenging scenes with strong global illumination. To fully make use of our deep neural network, we partition the scene space into an adaptive hierarchical grid, in which we apply our network to reconstruct high-quality sampling distributions for any local region in the scene. This allows for effective path guiding for arbitrary path bounce at any location in path tracing. We demonstrate that our photon-driven neural path guiding approach can generalize to diverse testing scenes, often achieving better rendering results than previous path guiding approaches and opening up interesting future directions.},
  langid = {english},
  file = {/Users/julianstamm/Zotero/storage/U54J5ZF5/Zhu et al. - 2022 - Photon-Driven Neural Reconstruction for Path Guiding.pdf}
}

@inreference{zordercurve2025,
  title = {Z-Order Curve},
  booktitle = {Wikipedia},
  date = {2025-02-08T19:21:58Z},
  url = {https://en.wikipedia.org/w/index.php?title=Z-order_curve&oldid=1274694704},
  urldate = {2025-05-30},
  abstract = {In mathematical analysis and computer science, functions which are Z-order, Lebesgue curve, Morton space-filling curve, Morton order or Morton code map multidimensional data to one dimension while preserving locality of the data points (two points close together in multidimensions with high probability lie also close together in Morton order). It is named in France after Henri Lebesgue, who studied it in 1904, and named in the United States after Guy Macdonald Morton, who first applied the order to file sequencing in 1966. The z-value of a point in multidimensions is simply calculated by bit interleaving the binary representations of its coordinate values. However, when querying a multidimensional search range in these data, using binary search is not really efficient: It is necessary for calculating, from a point encountered in the data structure, the next possible Z-value which is in the multidimensional search range, called BIGMIN. The BIGMIN problem has first been stated and its solution shown by Tropf and Herzog in 1981. Once the data are sorted by bit interleaving, any one-dimensional data structure can be used, such as simple one dimensional arrays, binary search trees, B-trees, skip lists or (with low significant bits truncated) hash tables. The resulting ordering can equivalently be described as the order one would get from a depth-first traversal of a quadtree or octree.},
  langid = {english},
  annotation = {Page Version ID: 1274694704},
  file = {/Users/julianstamm/Zotero/storage/3BWLCJ5L/index.html}
}
